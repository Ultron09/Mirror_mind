# Guide to the Consciousness v1.1.1 Module ("Sentient" Edition)

**Status:** Implemented | **Version:** 1.1.1 "Sentient"
**Module:** `airbornehrs.consciousness_v2`

---

## 1. Overview

The `consciousness_v2` module provides a high-level cognitive control layer for the MirrorMind framework. Its primary purpose is to analyze the model's performance on a given task and produce a `learning_multiplier` that dynamically adjusts the intensity of the training process.

It operates by simulating a simplified "emotional" response to the learning process. States like 'curiosity' (high uncertainty) or 'boredom' (low novelty) are used to determine whether to increase or decrease the learning rate for a given batch.

**Core Functionality:**

*   **Calculates Core Metrics:** Continuously tracks error, confidence, uncertainty, and surprise.
*   **Determines Emotional State:** Uses an `EmotionalSystem` to map the core metrics to one of seven emotional states.
*   **Modulates Learning:** Outputs a `learning_multiplier` to speed up or slow down learning based on the current emotional state.
*   **Stores Experiences:** Maintains an `EpisodicMemory` of recent learning events for potential future analysis.
*   **System 2 Reasoning (V8.0):** Uses a `RecursiveGlobalWorkspace` for multi-step "thinking" on complex problems.
*   **Thought Traces (V8.0):** Provides introspectable reasoning chains via `current_thought_trace`.
*   **Confusion Metric (V8.0):** Exposes `confusion` level for external use in learning rate adaptation.

---

## 2. The Core Workflow: The `observe` Method

The entire process is orchestrated by the `EnhancedConsciousnessCore.observe()` method. Here is what happens in a single call:

1.  **Compute Metrics:** Given the model's predictions and the ground truth, the method first calculates:
    *   **Error:** The loss for the current batch.
    *   **Confidence:** A measure of the model's certainty (inversely related to error).
    *   **Uncertainty:** The standard deviation of the model's predictions.
    *   **Surprise:** A Z-score indicating how much the current batch's error deviates from the recent moving average. A high surprise score means the model encountered something unexpected.

2.  **Determine Emotional State:** These metrics are fed into the `EmotionalSystem`. This component maps the metrics to the most dominant of seven possible `EmotionalState`s.

    ```python
    class EmotionalState(Enum):
        CONFIDENT = "confident"
        ANXIOUS = "anxious"
        CURIOUS = "curious"
        BORED = "bored"
        FRUSTRATED = "frustrated"
        SATISFIED = "satisfied"
        OVERWHELMED = "overwhelmed"
    ```

3.  **Get Learning Multiplier:** Based on the dominant emotional state, a `learning_multiplier` is selected. This multiplier is the key output of the consciousness module.

    | Emotion     | Learning Rate Multiplier | Rationale                                  |
    |-------------|--------------------------|--------------------------------------------|
    | Frustrated  | 1.8x                     | Desperate, aggressive learning to overcome a block. |
    | Anxious     | 1.4x                     | Heightened focus on difficult material.    |
    | Curious     | 1.3x                     | Motivated to explore novel inputs.         |
    | Confident   | 1.0x                     | Normal, steady learning pace.              |
    | Satisfied   | 1.0x                     | Consolidating knowledge effectively.       |
    | Bored       | 0.7x                     | Reduce effort on easy or repetitive material.|
    | Overwhelmed | 0.5x                     | Slow down to prevent divergence when lost. |

4.  **Store Episode:** The metrics and the resulting emotional state are packaged into a `MemoryEpisode` and stored in the `EpisodicMemory` buffer.

5.  **Return Data:** The `observe` method returns a dictionary containing the computed metrics and the learning multiplier, which can then be used in the training loop.

---

## 3. How to Use in Training

The `EnhancedConsciousnessCore` is designed to be integrated directly into your training loop. The `AdaptiveFramework` handles this automatically, but if you were to use it manually, the process would look like this:

```python
import torch
import torch.nn.functional as F
from airbornehrs.consciousness_v2 import EnhancedConsciousnessCore

# Initialize the consciousness module once before training
consciousness = EnhancedConsciousnessCore()

# --- Inside your training loop ---
for batch_idx, (x_batch, y_batch) in enumerate(train_loader):
    # 1. Get model output
    optimizer.zero_grad()
    logits = model(x_batch)
    loss = criterion(logits, y_batch)
    
    # Detach predictions for observation to prevent graph issues
    with torch.no_grad():
        y_pred = F.softmax(logits, dim=1)
    
    # 2. Let the consciousness module observe the step
    # This is the main interaction point.
    introspection_data = consciousness.observe(
        x=x_batch,
        y_true=y_batch,
        y_pred=y_pred
    )
    
    # 3. Get the learning multiplier from the returned data
    learning_multiplier = introspection_data['learning_multiplier']
    
    # 4. Apply the multiplier to the loss before backpropagation
    # This dynamically adjusts the learning intensity for the current batch.
    (loss * learning_multiplier).backward()
    
    # 5. Step the optimizer as usual
    optimizer.step()

    if batch_idx % 50 == 0:
        print(f"Step {batch_idx}, Emotion: {introspection_data['emotion']}, Multiplier: {learning_multiplier:.2f}")

```

---

## 4. API Reference

### `EnhancedConsciousnessCore`

This is the main class that integrates all components.

**Constructor:**
```python
core = EnhancedConsciousnessCore(
    feature_dim: int = 256,
    awareness_buffer_size: int = 5000,
    novelty_threshold: float = 2.0
)
```

**Main Method:**
```python
introspection_data = core.observe(
    x: torch.Tensor,
    y_true: torch.Tensor,
    y_pred: torch.Tensor,
    task_id: str = "default",
    features: Optional[torch.Tensor] = None
) -> Dict[str, Any]
```
-   **Description:** The primary method of the module. It takes the model's inputs, outputs, and ground truth for a single step, and returns a dictionary of analytics.
-   **Returns (`introspection_data`):** A dictionary containing key metrics about the learning step. The most important fields are:
    -   `'emotion'`: The dominant `EmotionalState` for the batch (e.g., `"anxious"`).
    -   `'learning_multiplier'`: The float multiplier to be applied to the loss (e.g., `1.4`).
    -   `'error'`: The calculated loss for the batch.
    -   `'surprise'`: The Z-score of the batch's error.
    -   `'confidence'`: The model's confidence level.
    -   `'uncertainty'`: The model's uncertainty level.
    -   `'importance'`: A composite score of surprise and learning gain.
    -   `'memory_lesson'`: A simplified lesson from `EpisodicMemory`.

---

## 5. Experimental and Future Components

The `consciousness_v2.py` file contains several other classes (`MetaCognition`, `SelfModel`, `Personality`, `AdaptiveAwareness`). While these classes are defined, they are **not actively used** by the `EnhancedConsciousnessCore.observe()` method in the current implementation.

They exist as placeholders for future development and experimentation. Their functionality is not exposed through the main consciousness API and should not be relied upon at this time. Their presence in the old guide was speculative. This guide documents the **current, working implementation**.

---

## 6. v1.1.1 "Sentient" Features

### 6.1. Recursive Global Workspace (System 2)

The v1.1.1 update introduces `RecursiveGlobalWorkspace`, which enables multi-step reasoning:

```python
# Accessing thought traces after observation
agent.consciousness.observe(y_true=y, y_pred=preds, features=features)
trace = agent.consciousness.current_thought_trace
print(f"Thought Depth: {len(trace)}")
```

### 6.2. Confusion Metric

The `confusion` metric (0.0 to 1.0) is now returned by `observe()`:

```python
metrics = agent.train_step(x, target_data=y)
confusion_level = metrics.get('confusion', 0.0)
# Higher confusion â†’ slower learning rate
```

### 6.3. SOTA Benchmarks

v1.1.1 has been verified with the following benchmarks (all passed):

| Test | Description |
| :--- | :--- |
| Few-Shot | >30% improvement in 10 shots |
| Forgetting | Task A retained after Task B |
| Noise | Stable under Gaussian noise |
| OOD Detection | Surprise=128.9 for OOD inputs |
| System 2 | Adaptive thought trace depth |