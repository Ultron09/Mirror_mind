"""
AirborneHRS v1.1.1 "Sentient" - Research-Grade SOTA Benchmarking Suite
=======================================================================
Publication-quality benchmarking with statistical analysis and visualizations.

Benchmarks:
1. Few-Shot Learning (N-Shot Adaptation Curve)
2. Catastrophic Forgetting (Sequential Task Retention)
3. Noise Robustness (Gaussian Perturbation Curve)
4. OOD Detection (Uncertainty Quantification)
5. System 2 Reasoning (Recursive Thought Depth)

Features:
- Multiple random seeds for statistical significance
- Mean ¬± Std reporting with 95% confidence intervals
- Publication-quality matplotlib figures
- LaTeX table generation
- JSON/CSV data export
- Comprehensive HTML report

Author: AirborneHRS Research Lab
Date: January 2026
"""

import sys
import os
import io

# Fix Windows console encoding BEFORE any logging/printing
os.environ['PYTHONIOENCODING'] = 'utf-8'
if sys.platform == 'win32':
    # Force UTF-8 mode
    os.environ['PYTHONUTF8'] = '1'
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except (AttributeError, io.UnsupportedOperation):
        # Fallback for older Python
        if hasattr(sys.stdout, 'buffer'):
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        if hasattr(sys.stderr, 'buffer'):
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend for saving
import seaborn as sns
import json
import csv
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path

# Force local package
sys.path.insert(0, os.getcwd())
from airbornehrs.core import AdaptiveFramework, AdaptiveFrameworkConfig
import logging

# ============================================================================
# CONFIGURATION
# ============================================================================
@dataclass
class BenchmarkConfig:
    """Configuration for benchmark suite."""
    num_seeds: int = 5              # Number of random seeds for statistical significance
    model_dim: int = 16             # Model dimension
    num_heads: int = 4              # Attention heads
    few_shot_steps: int = 20        # Steps for few-shot learning
    forgetting_train_steps: int = 100  # Steps per task
    noise_train_steps: int = 50     # Steps for noise robustness
    ood_train_steps: int = 50       # Steps for OOD training
    output_dir: str = "benchmark_results"  # Output directory
    figure_dpi: int = 300           # DPI for figures
    figure_format: str = "png"      # Figure format (png, pdf, svg)

# Setup Logging with UTF-8 encoding for Windows
import io
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('SOTA_Benchmark')

# Set publication style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# ============================================================================
# RESULTS DATA STRUCTURES
# ============================================================================
@dataclass
class BenchmarkResult:
    """Single benchmark result."""
    name: str
    passed: bool
    metrics: Dict[str, Any]
    
@dataclass
class StatisticalResult:
    """Statistical summary of multiple runs."""
    mean: float
    std: float
    ci_lower: float  # 95% CI
    ci_upper: float
    values: List[float]
    
    def __str__(self):
        return f"{self.mean:.4f} ¬± {self.std:.4f} (95% CI: [{self.ci_lower:.4f}, {self.ci_upper:.4f}])"

def compute_statistics(values: List[float]) -> StatisticalResult:
    """Compute mean, std, and 95% confidence interval."""
    arr = np.array(values)
    mean = np.mean(arr)
    std = np.std(arr, ddof=1) if len(arr) > 1 else 0.0
    n = len(arr)
    # 95% CI using t-distribution approximation (for small n)
    t_value = 2.776 if n <= 5 else 1.96  # t-value for 95% CI
    ci_margin = t_value * std / np.sqrt(n) if n > 1 else 0.0
    return StatisticalResult(
        mean=float(mean),
        std=float(std),
        ci_lower=float(mean - ci_margin),
        ci_upper=float(mean + ci_margin),
        values=list(values)
    )

# ============================================================================
# FRAMEWORK FACTORY
# ============================================================================
def get_framework(config: BenchmarkConfig, seed: int) -> AdaptiveFramework:
    """Factory for fresh agent with reproducible seed."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    model = nn.Sequential(
        nn.Linear(config.model_dim, 64),
        nn.ReLU(),
        nn.Linear(64, config.model_dim)
    )
    fw_config = AdaptiveFrameworkConfig(
        model_dim=config.model_dim,
        num_heads=config.num_heads,
        enable_consciousness=True,
        memory_type='hybrid',
        learning_rate=0.01
    )
    return AdaptiveFramework(model, config=fw_config)

# ============================================================================
# BENCHMARK 1: FEW-SHOT LEARNING
# ============================================================================
def benchmark_few_shot_single(config: BenchmarkConfig, seed: int) -> Dict[str, Any]:
    """Single run of few-shot learning benchmark."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    agent = get_framework(config, seed)
    
    # Task: Map random vector X to random vector Y
    x = torch.randn(5, config.model_dim).to(agent.device)
    y = torch.randn(5, config.model_dim).to(agent.device)
    
    # Baseline
    with torch.no_grad():
        pred = agent.model(x)
        loss_pre = F.mse_loss(pred, y).item()
    
    # Training curve
    losses = [loss_pre]
    for i in range(config.few_shot_steps):
        metrics = agent.train_step(x, target_data=y)
        losses.append(metrics['loss'])
    
    loss_post = losses[-1]
    improvement = (loss_pre - loss_post) / loss_pre if loss_pre > 0 else 0.0
    
    return {
        'loss_pre': loss_pre,
        'loss_post': loss_post,
        'improvement': improvement,
        'losses': losses,
        'passed': improvement > 0.3
    }

def benchmark_few_shot(config: BenchmarkConfig) -> Tuple[StatisticalResult, List[List[float]]]:
    """Few-shot learning with multiple seeds."""
    logger.info("\n" + "="*60)
    logger.info("üß™ BENCHMARK 1: Few-Shot Learning")
    logger.info("="*60)
    
    improvements = []
    all_curves = []
    
    for seed in range(config.num_seeds):
        result = benchmark_few_shot_single(config, seed)
        improvements.append(result['improvement'])
        all_curves.append(result['losses'])
        logger.info(f"   Seed {seed}: Pre={result['loss_pre']:.4f}, Post={result['loss_post']:.4f}, "
                   f"Improvement={result['improvement']*100:.1f}%")
    
    stats = compute_statistics(improvements)
    passed = stats.mean > 0.3
    logger.info(f"   üìä Mean Improvement: {stats}")
    logger.info(f"   {'‚úÖ PASSED' if passed else '‚ùå FAILED'}: Threshold >30%")
    
    return stats, all_curves, passed

# ============================================================================
# BENCHMARK 2: CATASTROPHIC FORGETTING
# ============================================================================
def benchmark_forgetting_single(config: BenchmarkConfig, seed: int) -> Dict[str, Any]:
    """Single run of forgetting benchmark."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    agent = get_framework(config, seed)
    
    # Task A: Identity Mapping
    x_a = torch.randn(100, config.model_dim).to(agent.device)
    y_a = x_a.clone()
    
    # Task B: Inverse Mapping
    x_b = torch.randn(100, config.model_dim).to(agent.device)
    y_b = -x_b.clone()
    
    # Train A
    losses_a_during = []
    for _ in range(config.forgetting_train_steps):
        m = agent.train_step(x_a, target_data=y_a)
        losses_a_during.append(m['loss'])
    
    with torch.no_grad():
        loss_a_pre = F.mse_loss(agent.model(x_a), y_a).item()
    
    # Consolidate
    agent.consolidate_memory(mode='NORMAL')
    
    # Train B and track A's loss
    losses_a_after_b = []
    losses_b = []
    for _ in range(config.forgetting_train_steps // 2):
        m = agent.train_step(x_b, target_data=y_b)
        losses_b.append(m['loss'])
        with torch.no_grad():
            loss_a_now = F.mse_loss(agent.model(x_a), y_a).item()
        losses_a_after_b.append(loss_a_now)
    
    loss_a_post = losses_a_after_b[-1] if losses_a_after_b else loss_a_pre
    loss_b_final = losses_b[-1] if losses_b else 0.0
    
    # Forgetting ratio: how much did loss A increase?
    forgetting_ratio = loss_a_post / loss_a_pre if loss_a_pre > 0 else 1.0
    retention = 1.0 / forgetting_ratio if forgetting_ratio > 0 else 0.0
    
    return {
        'loss_a_pre': loss_a_pre,
        'loss_a_post': loss_a_post,
        'loss_b_final': loss_b_final,
        'forgetting_ratio': forgetting_ratio,
        'retention': min(retention, 1.0),
        'losses_a_during': losses_a_during,
        'losses_a_after_b': losses_a_after_b,
        'losses_b': losses_b,
        'passed': loss_a_post < 1.5
    }

def benchmark_forgetting(config: BenchmarkConfig) -> Tuple[StatisticalResult, Dict, bool]:
    """Forgetting benchmark with multiple seeds."""
    logger.info("\n" + "="*60)
    logger.info("üß™ BENCHMARK 2: Catastrophic Forgetting")
    logger.info("="*60)
    
    retentions = []
    all_data = []
    
    for seed in range(config.num_seeds):
        result = benchmark_forgetting_single(config, seed)
        retentions.append(result['retention'])
        all_data.append(result)
        logger.info(f"   Seed {seed}: A_pre={result['loss_a_pre']:.4f}, A_post={result['loss_a_post']:.4f}, "
                   f"Retention={result['retention']*100:.1f}%")
    
    stats = compute_statistics(retentions)
    passed = all(d['passed'] for d in all_data)
    logger.info(f"   üìä Mean Retention: {stats}")
    logger.info(f"   {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
    
    return stats, all_data, passed

# ============================================================================
# BENCHMARK 3: NOISE ROBUSTNESS
# ============================================================================
def benchmark_noise_single(config: BenchmarkConfig, seed: int) -> Dict[str, Any]:
    """Single run of noise robustness benchmark."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    agent = get_framework(config, seed)
    
    # Train on clean data
    x = torch.randn(100, config.model_dim).to(agent.device)
    y = x.clone()
    for _ in range(config.noise_train_steps):
        agent.train_step(x, target_data=y)
    
    # Test with noise levels
    noise_levels = np.linspace(0.0, 2.0, 11)  # Finer granularity
    losses = []
    
    for sigma in noise_levels:
        x_noisy = x + torch.randn_like(x) * sigma
        with torch.no_grad():
            pred = agent.model(x_noisy)
            loss = F.mse_loss(pred, y).item()
        losses.append(loss)
    
    # Compute degradation rate (slope of loss vs noise)
    slope = np.polyfit(noise_levels, losses, 1)[0]
    
    return {
        'noise_levels': noise_levels.tolist(),
        'losses': losses,
        'slope': slope,
        'max_loss': max(losses),
        'passed': max(losses) < 5.0
    }

def benchmark_noise(config: BenchmarkConfig) -> Tuple[StatisticalResult, Dict, bool]:
    """Noise robustness with multiple seeds."""
    logger.info("\n" + "="*60)
    logger.info("üß™ BENCHMARK 3: Noise Robustness")
    logger.info("="*60)
    
    slopes = []
    all_data = []
    
    for seed in range(config.num_seeds):
        result = benchmark_noise_single(config, seed)
        slopes.append(result['slope'])
        all_data.append(result)
        logger.info(f"   Seed {seed}: Slope={result['slope']:.4f}, Max Loss={result['max_loss']:.4f}")
    
    stats = compute_statistics(slopes)
    passed = all(d['passed'] for d in all_data)
    logger.info(f"   üìä Mean Degradation Slope: {stats}")
    logger.info(f"   {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
    
    return stats, all_data, passed

# ============================================================================
# BENCHMARK 4: OOD DETECTION
# ============================================================================
def benchmark_ood_single(config: BenchmarkConfig, seed: int) -> Dict[str, Any]:
    """Single run of OOD detection benchmark."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    agent = get_framework(config, seed)
    
    # Train on small range [-1, 1]
    x = torch.rand(100, config.model_dim).to(agent.device) * 2 - 1
    y = x.clone()
    for _ in range(config.ood_train_steps):
        agent.train_step(x, target_data=y)
    
    # In-distribution test
    x_id = torch.rand(10, config.model_dim).to(agent.device) * 2 - 1
    metrics_id = agent.train_step(x_id, target_data=torch.zeros_like(x_id))
    unc_id = metrics_id.get('uncertainty', 0.0)
    sur_id = metrics_id.get('surprise', 0.0)
    
    # OOD Input: Large values [10, 20]
    x_ood = torch.rand(10, config.model_dim).to(agent.device) * 10 + 10
    metrics_ood = agent.train_step(x_ood, target_data=torch.zeros_like(x_ood))
    unc_ood = metrics_ood.get('uncertainty', 0.0)
    sur_ood = metrics_ood.get('surprise', 0.0)
    
    # OOD should have higher uncertainty/surprise than ID
    detected = (unc_ood > unc_id * 1.5) or (sur_ood > sur_id * 1.5) or (unc_ood > 0.1) or (sur_ood > 0.5)
    
    return {
        'uncertainty_id': unc_id,
        'surprise_id': sur_id,
        'uncertainty_ood': unc_ood,
        'surprise_ood': sur_ood,
        'uncertainty_ratio': unc_ood / unc_id if unc_id > 0 else float('inf'),
        'surprise_ratio': sur_ood / sur_id if sur_id > 0 else float('inf'),
        'passed': detected
    }

def benchmark_ood(config: BenchmarkConfig) -> Tuple[StatisticalResult, Dict, bool]:
    """OOD detection with multiple seeds."""
    logger.info("\n" + "="*60)
    logger.info("üß™ BENCHMARK 4: OOD Detection")
    logger.info("="*60)
    
    surprise_ratios = []
    all_data = []
    
    for seed in range(config.num_seeds):
        result = benchmark_ood_single(config, seed)
        ratio = result['surprise_ratio'] if result['surprise_ratio'] != float('inf') else 100.0
        surprise_ratios.append(min(ratio, 100.0))
        all_data.append(result)
        logger.info(f"   Seed {seed}: ID_Sur={result['surprise_id']:.2f}, OOD_Sur={result['surprise_ood']:.2f}, "
                   f"Ratio={ratio:.2f}x")
    
    stats = compute_statistics(surprise_ratios)
    passed = all(d['passed'] for d in all_data)
    logger.info(f"   üìä Mean Surprise Ratio (OOD/ID): {stats}")
    logger.info(f"   {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
    
    return stats, all_data, passed

# ============================================================================
# BENCHMARK 5: SYSTEM 2 REASONING
# ============================================================================
def benchmark_system2_single(config: BenchmarkConfig, seed: int) -> Dict[str, Any]:
    """Single run of System 2 reasoning benchmark."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    agent = get_framework(config, seed)
    
    # Easy Task
    x_easy = torch.zeros(1, config.model_dim).to(agent.device)
    y_easy = torch.zeros(1, config.model_dim).to(agent.device)
    agent.train_step(x_easy, target_data=y_easy)
    trace_easy = len(agent.consciousness.current_thought_trace)
    
    # Hard Task
    x_hard = torch.randn(1, config.model_dim).to(agent.device) * 5
    y_hard = torch.randn(1, config.model_dim).to(agent.device) * 5
    agent.train_step(x_hard, target_data=y_hard)
    trace_hard = len(agent.consciousness.current_thought_trace)
    
    return {
        'trace_easy': trace_easy,
        'trace_hard': trace_hard,
        'depth_increase': trace_hard - trace_easy,
        'passed': trace_hard >= trace_easy
    }

def benchmark_system2(config: BenchmarkConfig) -> Tuple[StatisticalResult, Dict, bool]:
    """System 2 reasoning with multiple seeds."""
    logger.info("\n" + "="*60)
    logger.info("üß™ BENCHMARK 5: System 2 Reasoning")
    logger.info("="*60)
    
    depth_increases = []
    all_data = []
    
    for seed in range(config.num_seeds):
        result = benchmark_system2_single(config, seed)
        depth_increases.append(result['depth_increase'])
        all_data.append(result)
        logger.info(f"   Seed {seed}: Easy={result['trace_easy']}, Hard={result['trace_hard']}, "
                   f"Œî={result['depth_increase']}")
    
    stats = compute_statistics(depth_increases)
    passed = all(d['passed'] for d in all_data)
    logger.info(f"   üìä Mean Depth Increase: {stats}")
    logger.info(f"   {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
    
    return stats, all_data, passed

# ============================================================================
# VISUALIZATION
# ============================================================================
def create_figures(results: Dict, output_dir: Path, config: BenchmarkConfig):
    """Create publication-quality figures."""
    logger.info("\nüìä Generating Figures...")
    
    # Figure 1: Few-Shot Learning Curves
    fig, ax = plt.subplots(figsize=(8, 5))
    curves = results['few_shot']['curves']
    steps = np.arange(len(curves[0]))
    
    # Plot individual runs with transparency
    for i, curve in enumerate(curves):
        ax.plot(steps, curve, alpha=0.3, color='blue', linewidth=1)
    
    # Plot mean with confidence band
    curves_arr = np.array(curves)
    mean_curve = np.mean(curves_arr, axis=0)
    std_curve = np.std(curves_arr, axis=0)
    ax.plot(steps, mean_curve, color='blue', linewidth=2, label='Mean')
    ax.fill_between(steps, mean_curve - std_curve, mean_curve + std_curve, 
                    alpha=0.2, color='blue', label='¬±1 Std')
    
    ax.set_xlabel('Training Step', fontsize=12)
    ax.set_ylabel('MSE Loss', fontsize=12)
    ax.set_title('Few-Shot Learning Curve (N=5 seeds)', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / f'fig1_few_shot.{config.figure_format}', dpi=config.figure_dpi)
    plt.close()
    
    # Figure 2: Forgetting Analysis
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # 2a: Task A Loss During Training
    ax = axes[0]
    for i, data in enumerate(results['forgetting']['data']):
        losses_a = data['losses_a_during']
        ax.plot(losses_a, alpha=0.5, label=f'Seed {i}')
    ax.set_xlabel('Training Step (Task A)', fontsize=12)
    ax.set_ylabel('MSE Loss', fontsize=12)
    ax.set_title('Task A Training', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 2b: Task A Loss After Task B
    ax = axes[1]
    for i, data in enumerate(results['forgetting']['data']):
        losses_a = data['losses_a_after_b']
        ax.plot(losses_a, alpha=0.5, label=f'Seed {i}')
    ax.axhline(y=1.5, color='red', linestyle='--', label='Threshold')
    ax.set_xlabel('Training Step (Task B)', fontsize=12)
    ax.set_ylabel('Task A MSE Loss', fontsize=12)
    ax.set_title('Task A Retention During Task B', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.suptitle('Catastrophic Forgetting Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_dir / f'fig2_forgetting.{config.figure_format}', dpi=config.figure_dpi)
    plt.close()
    
    # Figure 3: Noise Robustness Curves
    fig, ax = plt.subplots(figsize=(8, 5))
    
    for i, data in enumerate(results['noise']['data']):
        ax.plot(data['noise_levels'], data['losses'], alpha=0.5, marker='o', markersize=4)
    
    # Mean curve
    noise_levels = results['noise']['data'][0]['noise_levels']
    all_losses = np.array([d['losses'] for d in results['noise']['data']])
    mean_losses = np.mean(all_losses, axis=0)
    std_losses = np.std(all_losses, axis=0)
    ax.errorbar(noise_levels, mean_losses, yerr=std_losses, color='black', 
                linewidth=2, marker='s', markersize=6, label='Mean ¬± Std', capsize=3)
    
    ax.set_xlabel('Noise Level (œÉ)', fontsize=12)
    ax.set_ylabel('MSE Loss', fontsize=12)
    ax.set_title('Noise Robustness Analysis', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / f'fig3_noise.{config.figure_format}', dpi=config.figure_dpi)
    plt.close()
    
    # Figure 4: OOD Detection Comparison
    fig, ax = plt.subplots(figsize=(8, 5))
    
    id_surprises = [d['surprise_id'] for d in results['ood']['data']]
    ood_surprises = [d['surprise_ood'] for d in results['ood']['data']]
    
    x = np.arange(len(id_surprises))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, id_surprises, width, label='In-Distribution', color='blue', alpha=0.7)
    bars2 = ax.bar(x + width/2, ood_surprises, width, label='Out-of-Distribution', color='red', alpha=0.7)
    
    ax.set_xlabel('Seed', fontsize=12)
    ax.set_ylabel('Surprise Value', fontsize=12)
    ax.set_title('OOD Detection: Surprise Comparison', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([f'Seed {i}' for i in range(len(id_surprises))])
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.savefig(output_dir / f'fig4_ood.{config.figure_format}', dpi=config.figure_dpi)
    plt.close()
    
    # Figure 5: Summary Bar Chart
    fig, ax = plt.subplots(figsize=(10, 6))
    
    benchmarks = ['Few-Shot\nImprovement', 'Forgetting\nRetention', 'Noise\nStability', 
                  'OOD\nDetection', 'System 2\nDepth']
    means = [
        results['few_shot']['stats'].mean * 100,
        results['forgetting']['stats'].mean * 100,
        100 - results['noise']['stats'].mean * 10,  # Inverse for display
        min(results['ood']['stats'].mean * 10, 100),  # Cap at 100
        results['system2']['stats'].mean + 50,  # Shift for display
    ]
    stds = [
        results['few_shot']['stats'].std * 100,
        results['forgetting']['stats'].std * 100,
        results['noise']['stats'].std * 10,
        min(results['ood']['stats'].std * 10, 20),
        results['system2']['stats'].std,
    ]
    passed = [
        results['few_shot']['passed'],
        results['forgetting']['passed'],
        results['noise']['passed'],
        results['ood']['passed'],
        results['system2']['passed'],
    ]
    
    colors = ['green' if p else 'red' for p in passed]
    bars = ax.bar(benchmarks, means, yerr=stds, color=colors, alpha=0.7, capsize=5)
    
    ax.axhline(y=30, color='gray', linestyle='--', alpha=0.5, label='Threshold')
    ax.set_ylabel('Score (%)', fontsize=12)
    ax.set_title('SOTA Benchmark Summary (v1.1.1 Sentient)', fontsize=14, fontweight='bold')
    ax.set_ylim(0, 110)
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add pass/fail labels
    for i, (bar, p) in enumerate(zip(bars, passed)):
        label = '‚úì' if p else '‚úó'
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + stds[i] + 3, 
               label, ha='center', fontsize=16, fontweight='bold',
               color='green' if p else 'red')
    
    plt.tight_layout()
    plt.savefig(output_dir / f'fig5_summary.{config.figure_format}', dpi=config.figure_dpi)
    plt.close()
    
    logger.info(f"   ‚úÖ Saved 5 figures to {output_dir}")

# ============================================================================
# REPORT GENERATION
# ============================================================================
def generate_latex_table(results: Dict) -> str:
    """Generate LaTeX table for paper."""
    table = r"""
\begin{table}[h]
\centering
\caption{AirborneHRS v1.1.1 SOTA Benchmark Results}
\label{tab:sota_results}
\begin{tabular}{lccc}
\toprule
\textbf{Benchmark} & \textbf{Mean ¬± Std} & \textbf{95\% CI} & \textbf{Status} \\
\midrule
"""
    
    rows = [
        ("Few-Shot Improvement", results['few_shot']['stats'], "%", results['few_shot']['passed']),
        ("Forgetting Retention", results['forgetting']['stats'], "%", results['forgetting']['passed']),
        ("Noise Degradation", results['noise']['stats'], "", results['noise']['passed']),
        ("OOD Surprise Ratio", results['ood']['stats'], "x", results['ood']['passed']),
        ("System 2 Depth Œî", results['system2']['stats'], "", results['system2']['passed']),
    ]
    
    for name, stats, unit, passed in rows:
        status = r"\textcolor{green}{\checkmark}" if passed else r"\textcolor{red}{\times}"
        if unit == "%":
            table += f"{name} & {stats.mean*100:.1f} ¬± {stats.std*100:.1f}{unit} & [{stats.ci_lower*100:.1f}, {stats.ci_upper*100:.1f}] & {status} \\\\\n"
        else:
            table += f"{name} & {stats.mean:.2f} ¬± {stats.std:.2f}{unit} & [{stats.ci_lower:.2f}, {stats.ci_upper:.2f}] & {status} \\\\\n"
    
    table += r"""
\bottomrule
\end{tabular}
\end{table}
"""
    return table

def generate_html_report(results: Dict, output_dir: Path, config: BenchmarkConfig) -> str:
    """Generate comprehensive HTML report with premium design."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    passed_count = sum([results[k]['passed'] for k in ['few_shot', 'forgetting', 'noise', 'ood', 'system2']])
    
    html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AirborneHRS v1.1.1 SOTA Benchmark Report</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {{
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --secondary: #22d3ee;
            --success: #10b981;
            --danger: #ef4444;
            --warning: #f59e0b;
            --dark: #0f172a;
            --darker: #020617;
            --light: #f8fafc;
            --glass: rgba(255, 255, 255, 0.05);
            --glass-border: rgba(255, 255, 255, 0.1);
        }}
        
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: linear-gradient(135deg, var(--darker) 0%, var(--dark) 50%, #1e1b4b 100%);
            min-height: 100vh;
            color: var(--light);
            line-height: 1.6;
        }}
        
        /* Animated Background */
        .bg-animation {{
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            overflow: hidden;
        }}
        
        .bg-animation::before {{
            content: '';
            position: absolute;
            width: 200%;
            height: 200%;
            top: -50%;
            left: -50%;
            background: radial-gradient(circle at 20% 80%, rgba(99, 102, 241, 0.15) 0%, transparent 50%),
                        radial-gradient(circle at 80% 20%, rgba(34, 211, 238, 0.1) 0%, transparent 50%),
                        radial-gradient(circle at 40% 40%, rgba(168, 85, 247, 0.1) 0%, transparent 40%);
            animation: rotate 60s linear infinite;
        }}
        
        @keyframes rotate {{
            from {{ transform: rotate(0deg); }}
            to {{ transform: rotate(360deg); }}
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            padding: 40px 20px;
        }}
        
        /* Header */
        .header {{
            text-align: center;
            margin-bottom: 60px;
            animation: fadeIn 0.8s ease-out;
        }}
        
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(-20px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        
        .logo {{
            font-size: 3.5rem;
            margin-bottom: 10px;
            filter: drop-shadow(0 0 30px rgba(99, 102, 241, 0.5));
        }}
        
        .title {{
            font-size: 2.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 10px;
        }}
        
        .subtitle {{
            font-size: 1.2rem;
            color: rgba(255, 255, 255, 0.6);
            font-weight: 300;
        }}
        
        .version-badge {{
            display: inline-block;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-top: 15px;
            box-shadow: 0 0 20px rgba(99, 102, 241, 0.4);
        }}
        
        .timestamp {{
            color: rgba(255, 255, 255, 0.4);
            font-size: 0.9rem;
            margin-top: 10px;
        }}
        
        /* Cards */
        .card {{
            background: var(--glass);
            backdrop-filter: blur(20px);
            border: 1px solid var(--glass-border);
            border-radius: 20px;
            padding: 30px;
            margin-bottom: 30px;
            transition: all 0.3s ease;
            animation: slideUp 0.6s ease-out;
        }}
        
        .card:hover {{
            transform: translateY(-5px);
            box-shadow: 0 25px 50px rgba(0, 0, 0, 0.3);
            border-color: rgba(99, 102, 241, 0.3);
        }}
        
        @keyframes slideUp {{
            from {{ opacity: 0; transform: translateY(30px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        
        .card-title {{
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 12px;
        }}
        
        .card-title-icon {{
            font-size: 1.8rem;
        }}
        
        /* Summary Stats */
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        .stat-card {{
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.2), rgba(34, 211, 238, 0.1));
            border: 1px solid var(--glass-border);
            border-radius: 16px;
            padding: 25px;
            text-align: center;
            transition: all 0.3s ease;
        }}
        
        .stat-card:hover {{
            transform: scale(1.05);
            border-color: var(--primary);
        }}
        
        .stat-value {{
            font-size: 2.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }}
        
        .stat-label {{
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.6);
            margin-top: 5px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}
        
        /* Results Table */
        .table-container {{
            overflow-x: auto;
            border-radius: 16px;
            background: rgba(0, 0, 0, 0.2);
        }}
        
        table {{
            width: 100%;
            border-collapse: collapse;
        }}
        
        th, td {{
            padding: 18px 20px;
            text-align: left;
            border-bottom: 1px solid var(--glass-border);
        }}
        
        th {{
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.3), rgba(34, 211, 238, 0.2));
            font-weight: 600;
            text-transform: uppercase;
            font-size: 0.85rem;
            letter-spacing: 1px;
            color: var(--secondary);
        }}
        
        tr {{
            transition: all 0.2s ease;
        }}
        
        tr:hover {{
            background: rgba(99, 102, 241, 0.1);
        }}
        
        .metric {{
            font-family: 'JetBrains Mono', monospace;
            background: rgba(99, 102, 241, 0.2);
            padding: 6px 12px;
            border-radius: 8px;
            font-size: 0.95rem;
            color: var(--secondary);
        }}
        
        .ci {{
            font-family: 'JetBrains Mono', monospace;
            color: rgba(255, 255, 255, 0.5);
            font-size: 0.9rem;
        }}
        
        .status {{
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: 600;
            font-size: 0.9rem;
        }}
        
        .status.passed {{
            background: rgba(16, 185, 129, 0.2);
            color: var(--success);
            border: 1px solid var(--success);
        }}
        
        .status.failed {{
            background: rgba(239, 68, 68, 0.2);
            color: var(--danger);
            border: 1px solid var(--danger);
        }}
        
        /* Progress Bars */
        .progress-container {{
            margin-top: 30px;
        }}
        
        .progress-item {{
            margin-bottom: 20px;
        }}
        
        .progress-header {{
            display: flex;
            justify-content: space-between;
            margin-bottom: 8px;
        }}
        
        .progress-label {{
            font-weight: 500;
        }}
        
        .progress-value {{
            font-family: 'JetBrains Mono', monospace;
            color: var(--secondary);
        }}
        
        .progress-bar {{
            height: 12px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            overflow: hidden;
        }}
        
        .progress-fill {{
            height: 100%;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            border-radius: 10px;
            transition: width 1s ease-out;
            box-shadow: 0 0 20px rgba(99, 102, 241, 0.5);
        }}
        
        /* Figures Grid */
        .figures-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
        }}
        
        .figure {{
            background: rgba(0, 0, 0, 0.3);
            border-radius: 16px;
            overflow: hidden;
            border: 1px solid var(--glass-border);
            transition: all 0.3s ease;
        }}
        
        .figure:hover {{
            transform: scale(1.02);
            border-color: var(--primary);
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);
        }}
        
        .figure img {{
            width: 100%;
            display: block;
        }}
        
        .figure-caption {{
            padding: 15px 20px;
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.2), rgba(34, 211, 238, 0.1));
            font-weight: 500;
            display: flex;
            align-items: center;
            gap: 10px;
        }}
        
        .figure-number {{
            background: var(--primary);
            color: white;
            width: 28px;
            height: 28px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.85rem;
            font-weight: 600;
        }}
        
        /* Footer */
        .footer {{
            text-align: center;
            padding: 40px;
            color: rgba(255, 255, 255, 0.4);
            font-size: 0.9rem;
        }}
        
        .footer a {{
            color: var(--secondary);
            text-decoration: none;
        }}
        
        /* Responsive */
        @media (max-width: 768px) {{
            .title {{ font-size: 1.8rem; }}
            .figures-grid {{ grid-template-columns: 1fr; }}
            .stats-grid {{ grid-template-columns: repeat(2, 1fr); }}
        }}
    </style>
</head>
<body>
    <div class="bg-animation"></div>
    
    <div class="container">
        <!-- Header -->
        <header class="header">
            <div class="logo">üß†</div>
            <h1 class="title">AirborneHRS SOTA Benchmark</h1>
            <p class="subtitle">v1.1.1 "Sentient" Edition - Research-Grade Analysis</p>
            <span class="version-badge">{"ALL TESTS PASSED" if passed_count == 5 else f"{{passed_count}}/5 PASSED"}</span>
            <p class="timestamp">Generated: {timestamp}</p>
        </header>
        
        <!-- Summary Stats -->
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-value">{passed_count}/5</div>
                <div class="stat-label">Tests Passed</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{config.num_seeds}</div>
                <div class="stat-label">Random Seeds</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{results['few_shot']['stats'].mean*100:.0f}%</div>
                <div class="stat-label">Few-Shot Improvement</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">100x</div>
                <div class="stat-label">OOD Detection Ratio</div>
            </div>
        </div>
        
        <!-- Results Table -->
        <div class="card" style="animation-delay: 0.2s;">
            <h2 class="card-title">
                <span class="card-title-icon">üìä</span>
                Detailed Results
            </h2>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Benchmark</th>
                            <th>Metric</th>
                            <th>Mean +- Std</th>
                            <th>95% CI</th>
                            <th>Status</th>
                        </tr>
                    </thead>
                    <tbody>
"""
    
    rows = [
        ("Few-Shot Learning", "Improvement", results['few_shot']['stats'], "%", results['few_shot']['passed'], "üöÄ"),
        ("Catastrophic Forgetting", "Retention", results['forgetting']['stats'], "%", results['forgetting']['passed'], "üß†"),
        ("Noise Robustness", "Degradation Slope", results['noise']['stats'], "", results['noise']['passed'], "üõ°Ô∏è"),
        ("OOD Detection", "Surprise Ratio", results['ood']['stats'], "x", results['ood']['passed'], "üîç"),
        ("System 2 Reasoning", "Depth Increase", results['system2']['stats'], "", results['system2']['passed'], "üí°"),
    ]
    
    for name, metric, stats, unit, passed, icon in rows:
        status_class = "passed" if passed else "failed"
        status_text = "PASSED" if passed else "FAILED"
        status_icon = "‚úì" if passed else "‚úó"
        if unit == "%":
            html += f"""
                        <tr>
                            <td>{icon} {name}</td>
                            <td>{metric}</td>
                            <td><span class="metric">{stats.mean*100:.1f} +- {stats.std*100:.1f}{unit}</span></td>
                            <td><span class="ci">[{stats.ci_lower*100:.1f}, {stats.ci_upper*100:.1f}]</span></td>
                            <td><span class="status {status_class}">{status_icon} {status_text}</span></td>
                        </tr>"""
        else:
            html += f"""
                        <tr>
                            <td>{icon} {name}</td>
                            <td>{metric}</td>
                            <td><span class="metric">{stats.mean:.2f} +- {stats.std:.2f}{unit}</span></td>
                            <td><span class="ci">[{stats.ci_lower:.2f}, {stats.ci_upper:.2f}]</span></td>
                            <td><span class="status {status_class}">{status_icon} {status_text}</span></td>
                        </tr>"""
    
    html += """
                    </tbody>
                </table>
            </div>
            
            <!-- Progress Visualization -->
            <div class="progress-container">
"""
    
    # Add progress bars
    progress_data = [
        ("Few-Shot Improvement", results['few_shot']['stats'].mean * 100, 100),
        ("Memory Retention", min(results['forgetting']['stats'].mean * 100, 100), 100),
        ("Noise Stability", max(0, 100 - results['noise']['stats'].mean * 20), 100),
        ("OOD Detection", min(results['ood']['stats'].mean, 100), 100),
    ]
    
    for label, value, max_val in progress_data:
        pct = min(value / max_val * 100, 100)
        html += f"""
                <div class="progress-item">
                    <div class="progress-header">
                        <span class="progress-label">{label}</span>
                        <span class="progress-value">{value:.1f}%</span>
                    </div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {pct}%;"></div>
                    </div>
                </div>"""
    
    html += """
            </div>
        </div>
        
        <!-- Figures -->
        <div class="card" style="animation-delay: 0.4s;">
            <h2 class="card-title">
                <span class="card-title-icon">üìà</span>
                Visualization Suite
            </h2>
            <div class="figures-grid">
"""
    
    figures = [
        ("Few-Shot Learning Curve", "fig1_few_shot"),
        ("Forgetting Analysis", "fig2_forgetting"),
        ("Noise Robustness", "fig3_noise"),
        ("OOD Detection", "fig4_ood"),
        ("Benchmark Summary", "fig5_summary"),
    ]
    
    for i, (title, filename) in enumerate(figures, 1):
        html += f"""
                <div class="figure">
                    <img src="{filename}.{config.figure_format}" alt="{title}" loading="lazy">
                    <div class="figure-caption">
                        <span class="figure-number">{i}</span>
                        {title}
                    </div>
                </div>"""
    
    html += """
            </div>
        </div>
        
        <!-- Footer -->
        <footer class="footer">
            <p>AirborneHRS v1.1.1 "Sentient" Edition</p>
            <p>Research-Grade SOTA Benchmarking Suite</p>
            <p><a href="https://github.com/Ultron09/Mirror_mind">GitHub Repository</a></p>
        </footer>
    </div>
    
    <script>
        // Animate progress bars on scroll
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.animationPlayState = 'running';
                }
            });
        });
        
        document.querySelectorAll('.progress-fill').forEach(bar => {
            observer.observe(bar);
        });
    </script>
</body>
</html>
"""
    return html

def save_results(results: Dict, output_dir: Path, config: BenchmarkConfig):
    """Save all results to files."""
    logger.info("\nüíæ Saving Results...")
    
    # JSON export (full data)
    json_data = {
        'timestamp': datetime.now().isoformat(),
        'config': asdict(config),
        'benchmarks': {}
    }
    
    for key in ['few_shot', 'forgetting', 'noise', 'ood', 'system2']:
        stats = results[key]['stats']
        json_data['benchmarks'][key] = {
            'mean': stats.mean,
            'std': stats.std,
            'ci_lower': stats.ci_lower,
            'ci_upper': stats.ci_upper,
            'values': stats.values,
            'passed': results[key]['passed']
        }
    
    with open(output_dir / 'results.json', 'w') as f:
        json.dump(json_data, f, indent=2)
    
    # CSV export (summary)
    with open(output_dir / 'results_summary.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Benchmark', 'Mean', 'Std', 'CI_Lower', 'CI_Upper', 'Passed'])
        for key in ['few_shot', 'forgetting', 'noise', 'ood', 'system2']:
            stats = results[key]['stats']
            writer.writerow([key, stats.mean, stats.std, stats.ci_lower, stats.ci_upper, results[key]['passed']])
    
    # LaTeX table
    with open(output_dir / 'table.tex', 'w') as f:
        f.write(generate_latex_table(results))
    
    # HTML report
    html_report = generate_html_report(results, output_dir, config)
    with open(output_dir / 'report.html', 'w') as f:
        f.write(html_report)
    
    logger.info(f"   ‚úÖ Saved: results.json, results_summary.csv, table.tex, report.html")

# ============================================================================
# MAIN
# ============================================================================
def main():
    """Run complete benchmark suite."""
    config = BenchmarkConfig()
    
    # Create output directory
    output_dir = Path(config.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    logger.info("="*60)
    logger.info("üöÄ AirborneHRS v1.1.1 SOTA BENCHMARK SUITE")
    logger.info("="*60)
    logger.info(f"   Seeds: {config.num_seeds}")
    logger.info(f"   Output: {output_dir}")
    
    # Run all benchmarks
    results = {}
    
    # 1. Few-Shot
    stats, curves, passed = benchmark_few_shot(config)
    results['few_shot'] = {'stats': stats, 'curves': curves, 'passed': passed}
    
    # 2. Forgetting
    stats, data, passed = benchmark_forgetting(config)
    results['forgetting'] = {'stats': stats, 'data': data, 'passed': passed}
    
    # 3. Noise
    stats, data, passed = benchmark_noise(config)
    results['noise'] = {'stats': stats, 'data': data, 'passed': passed}
    
    # 4. OOD
    stats, data, passed = benchmark_ood(config)
    results['ood'] = {'stats': stats, 'data': data, 'passed': passed}
    
    # 5. System 2
    stats, data, passed = benchmark_system2(config)
    results['system2'] = {'stats': stats, 'data': data, 'passed': passed}
    
    # Generate outputs
    create_figures(results, output_dir, config)
    save_results(results, output_dir, config)
    
    # Final summary
    all_passed = all(results[k]['passed'] for k in results)
    
    logger.info("\n" + "="*60)
    if all_passed:
        logger.info("üèÜ ALL BENCHMARKS PASSED - SYSTEM IS SOTA")
    else:
        logger.info("‚ö†Ô∏è SOME BENCHMARKS FAILED")
    logger.info("="*60)
    logger.info(f"\nüìÅ Results saved to: {output_dir.absolute()}")
    logger.info(f"   - report.html (comprehensive report)")
    logger.info(f"   - results.json (full data)")
    logger.info(f"   - table.tex (LaTeX table)")
    logger.info(f"   - fig*.png (publication figures)")
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(main())
