{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv1XvYxr2BQw"
      },
      "outputs": [],
      "source": [
        "# Advanced Chess Transformer with Novel Architectures and Training Techniques\n",
        "# Publication-Grade Implementation with Multiple Innovations\n",
        "\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import chess\n",
        "import chess.engine\n",
        "from collections import deque, namedtuple\n",
        "import json\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Constants\n",
        "BOARD_SIZE = 8\n",
        "NUM_SQUARES = BOARD_SIZE * BOARD_SIZE  # 64\n",
        "ACTION_SPACE_SIZE = NUM_SQUARES * NUM_SQUARES  # 4096\n",
        "NUM_PIECE_TYPES = 13  # 0 = empty, 1-6 white pawn..king, 7-12 black pawn..king\n",
        "NUM_TOKENS = 16  # Extended for special tokens\n",
        "\n",
        "# Special tokens\n",
        "EMPTY_TOKEN = 0\n",
        "CLS_TOKEN = 13\n",
        "MASK_TOKEN = 14\n",
        "ENDGAME_TOKEN = 15\n",
        "\n",
        "# Game phase detection thresholds\n",
        "OPENING_MOVES = 15\n",
        "ENDGAME_MATERIAL = 13  # Total piece value threshold\n",
        "\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done', 'value', 'policy'])\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Configuration for training hyperparameters\"\"\"\n",
        "    d_model: int = 256\n",
        "    nhead: int = 16\n",
        "    num_layers: int = 8\n",
        "    dropout: float = 0.1\n",
        "    learning_rate: float = 3e-4\n",
        "    batch_size: int = 32\n",
        "    memory_size: int = 100000\n",
        "    target_update_freq: int = 1000\n",
        "    exploration_noise: float = 0.3\n",
        "    temperature: float = 1.0\n",
        "    lambda_value: float = 0.95\n",
        "    entropy_coeff: float = 0.01\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Learnable positional encoding with chess-specific geometry\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, max_len: int = 65):\n",
        "        super().__init__()\n",
        "        # Standard positional encoding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        # Chess-specific encodings\n",
        "        self.rank_embed = nn.Embedding(8, d_model // 4)  # Rank (1-8)\n",
        "        self.file_embed = nn.Embedding(8, d_model // 4)  # File (a-h)\n",
        "        self.diagonal_embed = nn.Embedding(15, d_model // 4)  # Diagonals\n",
        "        self.color_embed = nn.Embedding(2, d_model // 4)  # Square color\n",
        "\n",
        "    def forward(self, positions):\n",
        "        batch_size, seq_len = positions.shape\n",
        "\n",
        "        # Standard positional encoding\n",
        "        pos_enc = self.pos_embed(positions)\n",
        "\n",
        "        # Chess-specific encodings for board squares (skip CLS token)\n",
        "        board_positions = positions[:, 1:]  # Skip CLS token\n",
        "\n",
        "        # Compute rank, file, diagonal, and color for each square\n",
        "        ranks = board_positions // 8\n",
        "        files = board_positions % 8\n",
        "        diagonals = ranks + files  # Main diagonal encoding\n",
        "        colors = (ranks + files) % 2  # Square color (0=dark, 1=light)\n",
        "\n",
        "        # Get embeddings\n",
        "        rank_enc = self.rank_embed(ranks)\n",
        "        file_enc = self.file_embed(files)\n",
        "        diag_enc = self.diagonal_embed(diagonals)\n",
        "        color_enc = self.color_embed(colors)\n",
        "\n",
        "        # Combine chess-specific encodings\n",
        "        chess_enc = torch.cat([rank_enc, file_enc, diag_enc, color_enc], dim=-1)\n",
        "\n",
        "        # Add zero encoding for CLS token\n",
        "        cls_enc = torch.zeros(batch_size, 1, self.d_model, device=positions.device)\n",
        "        full_chess_enc = torch.cat([cls_enc, chess_enc], dim=1)\n",
        "\n",
        "        return pos_enc + full_chess_enc\n",
        "\n",
        "class MultiScaleAttention(nn.Module):\n",
        "    \"\"\"Multi-scale attention for capturing both local and global patterns\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.local_attention = nn.MultiheadAttention(d_model, num_heads // 2, batch_first=True)\n",
        "        self.global_attention = nn.MultiheadAttention(d_model, num_heads // 2, batch_first=True)\n",
        "        self.combine = nn.Linear(2 * d_model, d_model)\n",
        "\n",
        "    def create_local_mask(self, seq_len: int, window_size: int = 5):\n",
        "        \"\"\"Create attention mask for local patterns (nearby squares)\"\"\"\n",
        "        mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
        "        for i in range(1, seq_len):  # Skip CLS token\n",
        "            square = i - 1\n",
        "            rank, file = square // 8, square % 8\n",
        "\n",
        "            # Allow attention to nearby squares\n",
        "            for j in range(1, seq_len):\n",
        "                other_square = j - 1\n",
        "                other_rank, other_file = other_square // 8, other_square % 8\n",
        "\n",
        "                distance = max(abs(rank - other_rank), abs(file - other_file))\n",
        "                if distance <= window_size:\n",
        "                    mask[i, j] = True\n",
        "\n",
        "        # CLS token can attend to all\n",
        "        mask[0, :] = True\n",
        "        mask[:, 0] = True\n",
        "\n",
        "        return ~mask  # Invert for attention mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Local attention with restricted window\n",
        "        local_mask = self.create_local_mask(seq_len).to(x.device)\n",
        "        local_out, _ = self.local_attention(x, x, x, attn_mask=local_mask)\n",
        "\n",
        "        # Global attention (unrestricted)\n",
        "        global_out, _ = self.global_attention(x, x, x)\n",
        "\n",
        "        # Combine and project\n",
        "        combined = torch.cat([local_out, global_out], dim=-1)\n",
        "        return self.combine(combined)\n",
        "\n",
        "class ChessTransformerBlock(nn.Module):\n",
        "    \"\"\"Enhanced transformer block with chess-specific components\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiScaleAttention(d_model, nhead)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Enhanced feed-forward with gating\n",
        "        self.ff1 = nn.Linear(d_model, 4 * d_model)\n",
        "        self.ff2 = nn.Linear(4 * d_model, d_model)\n",
        "        self.gate = nn.Linear(d_model, 4 * d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-scale attention\n",
        "        attn_out = self.attention(x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Gated feed-forward\n",
        "        ff_out = self.ff1(x)\n",
        "        gate_out = torch.sigmoid(self.gate(x))\n",
        "        ff_out = ff_out * gate_out\n",
        "        ff_out = self.dropout(F.gelu(ff_out))\n",
        "        ff_out = self.ff2(ff_out)\n",
        "\n",
        "        return self.norm2(x + self.dropout(ff_out))\n",
        "\n",
        "class GamePhaseEncoder(nn.Module):\n",
        "    \"\"\"Encode game phase information (opening/middlegame/endgame)\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.phase_embed = nn.Embedding(3, d_model)  # 0=opening, 1=middle, 2=endgame\n",
        "        self.move_embed = nn.Embedding(200, d_model)  # Move number encoding\n",
        "        self.material_proj = nn.Linear(1, d_model)\n",
        "\n",
        "    def detect_phase(self, board_tensor, move_count):\n",
        "        \"\"\"Detect game phase based on material and move count\"\"\"\n",
        "        # Count material (simple heuristic)\n",
        "        material_count = torch.sum(board_tensor > 0, dim=1).float()\n",
        "\n",
        "        phase = torch.zeros(board_tensor.shape[0], dtype=torch.long, device=board_tensor.device)\n",
        "\n",
        "        # Opening phase\n",
        "        opening_mask = move_count < OPENING_MOVES\n",
        "        phase[opening_mask] = 0\n",
        "\n",
        "        # Endgame phase\n",
        "        endgame_mask = material_count < ENDGAME_MATERIAL\n",
        "        phase[endgame_mask] = 2\n",
        "\n",
        "        # Middle game (default)\n",
        "        middle_mask = ~(opening_mask | endgame_mask)\n",
        "        phase[middle_mask] = 1\n",
        "\n",
        "        return phase\n",
        "\n",
        "    def forward(self, board_tensor, move_count):\n",
        "        batch_size = board_tensor.shape[0]\n",
        "\n",
        "        # Detect game phase\n",
        "        phase = self.detect_phase(board_tensor, move_count)\n",
        "        phase_enc = self.phase_embed(phase)\n",
        "\n",
        "        # Move count encoding\n",
        "        move_count_clamped = torch.clamp(move_count, 0, 199).long()\n",
        "        move_enc = self.move_embed(move_count_clamped)\n",
        "\n",
        "        # Material encoding\n",
        "        material = torch.sum(board_tensor > 0, dim=1, keepdim=True).float()\n",
        "        material_enc = self.material_proj(material / 32.0)  # Normalize\n",
        "\n",
        "        return phase_enc + move_enc + material_enc\n",
        "\n",
        "class AdvancedChessTransformer(nn.Module):\n",
        "    \"\"\"Advanced Chess Transformer with multiple innovations\"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.d_model = config.d_model\n",
        "\n",
        "        # Token and positional embeddings\n",
        "        self.token_embed = nn.Embedding(NUM_TOKENS, config.d_model)\n",
        "        self.pos_encoding = PositionalEncoding(config.d_model)\n",
        "        self.phase_encoder = GamePhaseEncoder(config.d_model)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            ChessTransformerBlock(config.d_model, config.nhead, config.dropout)\n",
        "            for _ in range(config.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output heads with separate processing\n",
        "        self.policy_head = nn.ModuleList([\n",
        "            nn.Linear(config.d_model, config.d_model),\n",
        "            nn.LayerNorm(config.d_model),\n",
        "            nn.Linear(config.d_model, ACTION_SPACE_SIZE)\n",
        "        ])\n",
        "\n",
        "        self.value_head = nn.ModuleList([\n",
        "            nn.Linear(config.d_model, config.d_model // 2),\n",
        "            nn.LayerNorm(config.d_model // 2),\n",
        "            nn.Linear(config.d_model // 2, 1)\n",
        "        ])\n",
        "\n",
        "        # Auxiliary heads for self-supervised learning\n",
        "        self.piece_prediction_head = nn.Linear(config.d_model, NUM_PIECE_TYPES)\n",
        "        self.phase_prediction_head = nn.Linear(config.d_model, 3)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights with better initialization\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                nn.init.normal_(module.weight, 0, 0.02)\n",
        "\n",
        "    def forward(self, board_tensor, move_count=None, return_aux=False):\n",
        "        batch_size, seq_len = board_tensor.shape\n",
        "\n",
        "        # Default move count if not provided\n",
        "        if move_count is None:\n",
        "            move_count = torch.zeros(batch_size, device=board_tensor.device)\n",
        "\n",
        "        # Add CLS token\n",
        "        cls_tokens = torch.full((batch_size, 1), CLS_TOKEN, dtype=torch.long, device=board_tensor.device)\n",
        "        x = torch.cat([cls_tokens, board_tensor], dim=1)\n",
        "\n",
        "        # Create position indices\n",
        "        pos_ids = torch.arange(seq_len + 1, device=board_tensor.device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        # Embeddings\n",
        "        x = self.token_embed(x) + self.pos_encoding(pos_ids)\n",
        "\n",
        "        # Add game phase information\n",
        "        phase_info = self.phase_encoder(board_tensor, move_count)\n",
        "        x[:, 0] += phase_info  # Add to CLS token\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Extract CLS token representation\n",
        "        cls_repr = x[:, 0]\n",
        "\n",
        "        # Policy head\n",
        "        policy = cls_repr\n",
        "        for layer in self.policy_head[:-1]:\n",
        "            if isinstance(layer, nn.LayerNorm):\n",
        "                policy = layer(policy)\n",
        "            else:\n",
        "                policy = F.gelu(layer(policy))\n",
        "        policy_logits = self.policy_head[-1](self.dropout(policy))\n",
        "\n",
        "        # Value head\n",
        "        value = cls_repr\n",
        "        for layer in self.value_head[:-1]:\n",
        "            if isinstance(layer, nn.LayerNorm):\n",
        "                value = layer(value)\n",
        "            else:\n",
        "                value = F.gelu(layer(value))\n",
        "        value = self.value_head[-1](self.dropout(value)).squeeze(-1)\n",
        "\n",
        "        if return_aux:\n",
        "            # Auxiliary predictions for self-supervised learning\n",
        "            piece_logits = self.piece_prediction_head(x[:, 1:])  # Skip CLS\n",
        "            phase_logits = self.phase_prediction_head(cls_repr)\n",
        "            return policy_logits, value, piece_logits, phase_logits\n",
        "\n",
        "        return policy_logits, value\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"Prioritized Experience Replay with improved sampling\"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int, alpha: float = 0.6, beta: float = 0.4):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
        "        self.position = 0\n",
        "        self.max_priority = 1.0\n",
        "\n",
        "    def add(self, experience: Experience):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "        else:\n",
        "            self.buffer[self.position] = experience\n",
        "\n",
        "        # Assign max priority to new experience\n",
        "        self.priorities[self.position] = self.max_priority\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        if len(self.buffer) == 0:\n",
        "            return None\n",
        "\n",
        "        # Calculate sampling probabilities\n",
        "        priorities = self.priorities[:len(self.buffer)]\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        # Sample indices\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities, replace=True)\n",
        "\n",
        "        # Calculate importance sampling weights\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        # Get experiences\n",
        "        experiences = [self.buffer[i] for i in indices]\n",
        "\n",
        "        return experiences, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "\n",
        "class ChessEnvironment:\n",
        "    \"\"\"Enhanced Chess Environment with game analysis\"\"\"\n",
        "\n",
        "    def __init__(self, use_engine=False, engine_depth=5):\n",
        "        self.board = chess.Board()\n",
        "        self.move_history = []\n",
        "        self.use_engine = use_engine\n",
        "        self.engine_depth = engine_depth\n",
        "        self.engine = None\n",
        "\n",
        "        if use_engine:\n",
        "            try:\n",
        "                # Try to use Stockfish if available\n",
        "                self.engine = chess.engine.SimpleEngine.popen_uci(\"/usr/local/bin/stockfish\")\n",
        "            except:\n",
        "                print(\"Stockfish not found, continuing without engine evaluation\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.board.reset()\n",
        "        self.move_history = []\n",
        "        return self.board_to_tensor(), self.get_move_count()\n",
        "\n",
        "    def board_to_tensor(self):\n",
        "        \"\"\"Convert board to tensor with improved encoding\"\"\"\n",
        "        tensor = [EMPTY_TOKEN] * NUM_SQUARES\n",
        "        for square in chess.SQUARES:\n",
        "            piece = self.board.piece_at(square)\n",
        "            if piece is not None:\n",
        "                piece_type = piece.piece_type\n",
        "                color = piece.color\n",
        "                token = piece_type if color else piece_type + 6\n",
        "                tensor[square] = token\n",
        "        return torch.tensor(tensor, dtype=torch.long, device=device)\n",
        "\n",
        "    def get_move_count(self):\n",
        "        return len(self.move_history)\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        from_sq = action_idx // NUM_SQUARES\n",
        "        to_sq = action_idx % NUM_SQUARES\n",
        "\n",
        "        # Handle promotion\n",
        "        promotion = None\n",
        "        piece = self.board.piece_type_at(from_sq)\n",
        "        if piece == chess.PAWN:\n",
        "            if self.board.turn == chess.WHITE and to_sq // 8 == 7:\n",
        "                promotion = chess.QUEEN\n",
        "            elif self.board.turn == chess.BLACK and to_sq // 8 == 0:\n",
        "                promotion = chess.QUEEN\n",
        "\n",
        "        move = chess.Move(from_sq, to_sq, promotion=promotion)\n",
        "\n",
        "        if move in self.board.legal_moves:\n",
        "            self.board.push(move)\n",
        "            self.move_history.append(move)\n",
        "            reward = self.calculate_reward()\n",
        "        else:\n",
        "            # Penalty for illegal moves\n",
        "            reward = -1.0\n",
        "            # Make random legal move as fallback\n",
        "            legal_moves = list(self.board.legal_moves)\n",
        "            if legal_moves:\n",
        "                move = random.choice(legal_moves)\n",
        "                self.board.push(move)\n",
        "                self.move_history.append(move)\n",
        "\n",
        "        done = self.board.is_game_over()\n",
        "        return self.board_to_tensor(), self.get_move_count(), reward, done\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        \"\"\"Calculate reward based on multiple factors\"\"\"\n",
        "        if self.board.is_checkmate():\n",
        "            return 10.0 if self.board.turn == chess.BLACK else -10.0\n",
        "        elif self.board.is_stalemate() or self.board.is_insufficient_material():\n",
        "            return 0.0\n",
        "        elif self.board.is_check():\n",
        "            return 0.5 if self.board.turn == chess.BLACK else -0.5\n",
        "\n",
        "        # Small reward for making legal moves\n",
        "        return 0.01\n",
        "\n",
        "    def get_engine_evaluation(self):\n",
        "        \"\"\"Get engine evaluation if available\"\"\"\n",
        "        if self.engine is None:\n",
        "            return None\n",
        "        try:\n",
        "            info = self.engine.analyse(self.board, chess.engine.Limit(depth=self.engine_depth))\n",
        "            return info[\"score\"].relative.score(mate_score=10000) / 100.0\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.engine:\n",
        "            self.engine.quit()\n",
        "\n",
        "class AdvancedTrainer:\n",
        "    \"\"\"Advanced training system with multiple techniques\"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.model = AdvancedChessTransformer(config).to(device)\n",
        "        self.target_model = AdvancedChessTransformer(config).to(device)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=1e-4\n",
        "        )\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            self.optimizer, T_max=1000, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        self.replay_buffer = PrioritizedReplayBuffer(config.memory_size)\n",
        "        self.env = ChessEnvironment()\n",
        "\n",
        "        # Training statistics\n",
        "        self.stats = {\n",
        "            'games_played': 0,\n",
        "            'total_rewards': [],\n",
        "            'policy_losses': [],\n",
        "            'value_losses': [],\n",
        "            'aux_losses': []\n",
        "        }\n",
        "\n",
        "    def select_move(self, board_tensor, move_count, training=True, temperature=1.0):\n",
        "        \"\"\"Select move with improved exploration\"\"\"\n",
        "        self.model.eval() if not training else self.model.train()\n",
        "\n",
        "        with torch.no_grad() if not training else torch.enable_grad():\n",
        "            batch_tensor = board_tensor.unsqueeze(0)\n",
        "            batch_count = torch.tensor([move_count], device=device)\n",
        "\n",
        "            policy_logits, value = self.model(batch_tensor, batch_count)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                policy_logits = policy_logits / temperature\n",
        "\n",
        "            # Mask illegal moves\n",
        "            legal_moves = list(self.env.board.legal_moves)\n",
        "            if not legal_moves:\n",
        "                return None, None, None, None\n",
        "\n",
        "            legal_indices = []\n",
        "            for move in legal_moves:\n",
        "                if move.promotion and move.promotion != chess.QUEEN:\n",
        "                    continue\n",
        "                idx = move.from_square * NUM_SQUARES + move.to_square\n",
        "                legal_indices.append(idx)\n",
        "\n",
        "            if not legal_indices:\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Create mask\n",
        "            mask = torch.full((ACTION_SPACE_SIZE,), -1e9, device=device)\n",
        "            mask[legal_indices] = 0\n",
        "\n",
        "            masked_logits = policy_logits.squeeze(0) + mask\n",
        "\n",
        "            # Sample action\n",
        "            if training and random.random() < self.config.exploration_noise:\n",
        "                # Random exploration\n",
        "                action = random.choice(legal_indices)\n",
        "                probs = F.softmax(masked_logits, dim=-1)\n",
        "                log_prob = torch.log(probs[action] + 1e-8)\n",
        "            else:\n",
        "                # Policy sampling\n",
        "                probs = F.softmax(masked_logits, dim=-1)\n",
        "                m = Categorical(probs)\n",
        "                action = m.sample()\n",
        "                log_prob = m.log_prob(action)\n",
        "                action = action.item()\n",
        "\n",
        "            return action, log_prob, value.squeeze(0), masked_logits\n",
        "\n",
        "    def play_game(self, training=True):\n",
        "        \"\"\"Play a single game with experience collection\"\"\"\n",
        "        state, move_count = self.env.reset()\n",
        "        done = False\n",
        "        experiences = []\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done and len(experiences) < 200:  # Max game length\n",
        "            action, log_prob, value, policy_logits = self.select_move(\n",
        "                state, move_count, training, self.config.temperature\n",
        "            )\n",
        "\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            next_state, next_move_count, reward, done = self.env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if training:\n",
        "                experience = Experience(\n",
        "                    state=state.cpu(),\n",
        "                    action=action,\n",
        "                    reward=reward,\n",
        "                    next_state=next_state.cpu(),\n",
        "                    done=done,\n",
        "                    value=value.cpu() if value is not None else torch.tensor(0.0),\n",
        "                    policy=policy_logits.cpu() if policy_logits is not None else torch.zeros(ACTION_SPACE_SIZE)\n",
        "                )\n",
        "                experiences.append(experience)\n",
        "\n",
        "            state = next_state\n",
        "            move_count = next_move_count\n",
        "\n",
        "        # Add experiences to replay buffer with TD-lambda returns\n",
        "        if training and experiences:\n",
        "            returns = self.calculate_td_lambda_returns(experiences)\n",
        "            for exp, ret in zip(experiences, returns):\n",
        "                updated_exp = Experience(\n",
        "                    exp.state, exp.action, ret, exp.next_state,\n",
        "                    exp.done, exp.value, exp.policy\n",
        "                )\n",
        "                self.replay_buffer.add(updated_exp)\n",
        "\n",
        "        return total_reward, len(experiences)\n",
        "\n",
        "    def calculate_td_lambda_returns(self, experiences):\n",
        "        \"\"\"Calculate TD-lambda returns for better value estimation\"\"\"\n",
        "        returns = []\n",
        "        g = 0\n",
        "\n",
        "        for i in reversed(range(len(experiences))):\n",
        "            exp = experiences[i]\n",
        "            if exp.done:\n",
        "                g = exp.reward\n",
        "            else:\n",
        "                g = exp.reward + 0.99 * (self.config.lambda_value * g + (1 - self.config.lambda_value) * exp.value.item())\n",
        "            returns.insert(0, g)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def train_batch(self):\n",
        "        \"\"\"Train on a batch of experiences\"\"\"\n",
        "        if len(self.replay_buffer.buffer) < self.config.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample batch\n",
        "        batch_data = self.replay_buffer.sample(self.config.batch_size)\n",
        "        if batch_data is None:\n",
        "            return\n",
        "\n",
        "        experiences, indices, weights = batch_data\n",
        "\n",
        "        # Prepare batch tensors\n",
        "        states = torch.stack([exp.state for exp in experiences]).to(device)\n",
        "        actions = torch.tensor([exp.action for exp in experiences], device=device)\n",
        "        rewards = torch.tensor([exp.reward for exp in experiences], device=device, dtype=torch.float)\n",
        "        values = torch.stack([exp.value for exp in experiences]).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        policy_logits, pred_values, piece_logits, phase_logits = self.model(\n",
        "            states,\n",
        "            torch.zeros(len(experiences), device=device),\n",
        "            return_aux=True\n",
        "        )\n",
        "\n",
        "        # Policy loss\n",
        "        policy_dist = Categorical(logits=policy_logits)\n",
        "        log_probs = policy_dist.log_prob(actions)\n",
        "        advantages = rewards - pred_values.detach()\n",
        "        policy_loss = -(log_probs * advantages).mean()\n",
        "\n",
        "        # Value loss\n",
        "        value_loss = F.mse_loss(pred_values, rewards)\n",
        "\n",
        "        # Auxiliary losses for better representation learning\n",
        "        # Piece prediction loss (self-supervised)\n",
        "        piece_targets = states  # Predict the pieces on the board\n",
        "        aux_loss = F.cross_entropy(\n",
        "            piece_logits.reshape(-1, NUM_PIECE_TYPES),\n",
        "            piece_targets.reshape(-1),\n",
        "            ignore_index=EMPTY_TOKEN\n",
        "        )\n",
        "\n",
        "        # Entropy regularization\n",
        "        entropy_loss = -policy_dist.entropy().mean()\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = (policy_loss +\n",
        "                     value_loss +\n",
        "                     0.1 * aux_loss +\n",
        "                     self.config.entropy_coeff * entropy_loss)\n",
        "\n",
        "        # Apply importance sampling weights\n",
        "        weights_tensor = torch.tensor(weights, device=device, dtype=torch.float)\n",
        "        total_loss = (total_loss * weights_tensor).mean()\n",
        "\n",
        "        # Backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Update priorities\n",
        "        td_errors = torch.abs(rewards - pred_values).detach().cpu().numpy()\n",
        "        new_priorities = td_errors + 1e-6\n",
        "        self.replay_buffer.update_priorities(indices, new_priorities)\n",
        "\n",
        "        # Update statistics\n",
        "        self.stats['policy_losses'].append(policy_loss.item())\n",
        "        self.stats['value_losses'].append(value_loss.item())\n",
        "        self.stats['aux_losses'].append(aux_loss.item())\n",
        "\n",
        "    def train(self, num_games=1000, save_interval=100):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(f\"Starting training with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
        "\n",
        "        for game in range(1, num_games + 1):\n",
        "            # Play game and collect experience\n",
        "            reward, moves = self.play_game(training=True)\n",
        "            self.stats['games_played'] += 1\n",
        "            self.stats['total_rewards'].append(reward)\n",
        "\n",
        "            # Train on batch\n",
        "            if game % 4 == 0:  # Train every 4 games\n",
        "                for _ in range(2):  # Multiple training steps\n",
        "                    self.train_batch()\n",
        "\n",
        "            # Update target network\n",
        "            if game % self.config.target_update_freq == 0:\n",
        "                self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "            # Logging\n",
        "            if game % 10 == 0:\n",
        "                avg_reward = np.mean(self.stats['total_rewards'][-10:])\n",
        "                avg_policy_loss = np.mean(self.stats['policy_losses'][-10:]) if self.stats['policy_losses'] else 0\n",
        "                avg_value_loss = np.mean(self.stats['value_losses'][-10:]) if self.stats['value_losses'] else 0\n",
        "\n",
        "                print(f\"Game {game}: Reward={avg_reward:.3f}, \"\n",
        "                      f\"Policy Loss={avg_policy_loss:.4f}, \"\n",
        "                      f\"Value Loss={avg_value_loss:.4f}, \"\n",
        "                      f\"Moves={moves}\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            if game % save_interval == 0:\n",
        "                self.save_checkpoint(f\"advanced_chess_model_game_{game}.pth\")\n",
        "                print(f\"Checkpoint saved at game {game}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'config': self.config,\n",
        "            'stats': self.stats\n",
        "        }, path)\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        \"\"\"Load training checkpoint\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.stats = checkpoint['stats']\n",
        "        print(f\"Loaded checkpoint from {path}\")\n",
        "\n",
        "def play_against_human(model_path, config=None):\n",
        "    \"\"\"Play against the trained model\"\"\"\n",
        "    if config is None:\n",
        "        config = TrainingConfig()\n",
        "\n",
        "    # Load model\n",
        "    model = AdvancedChessTransformer(config).to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    env = ChessEnvironment()\n",
        "    board_state, move_count = env.reset()\n",
        "\n",
        "    print(\"ðŸš€ Advanced Chess AI - Ready to play!\")\n",
        "    print(\"You are White. Enter moves in UCI format (e.g., e2e4)\")\n",
        "    print(\"Type 'quit' to exit, 'hint' for AI suggestion\")\n",
        "\n",
        "    while not env.board.is_game_over():\n",
        "        print(f\"\\n{env.board}\")\n",
        "        print(f\"Move {move_count + 1}\")\n",
        "\n",
        "        if env.board.turn == chess.WHITE:  # Human turn\n",
        "            move_input = input(\"Your move: \").strip().lower()\n",
        "\n",
        "            if move_input == 'quit':\n",
        "                break\n",
        "            elif move_input == 'hint':\n",
        "                # Get AI suggestion\n",
        "                with torch.no_grad():\n",
        "                    policy_logits, value = model(board_state.unsqueeze(0), torch.tensor([move_count]))\n",
        "\n",
        "                    # Find best legal move\n",
        "                    legal_moves = list(env.board.legal_moves)\n",
        "                    best_score = -float('inf')\n",
        "                    best_move = None\n",
        "\n",
        "                    for move in legal_moves:\n",
        "                        if move.promotion and move.promotion != chess.QUEEN:\n",
        "                            continue\n",
        "                        idx = move.from_square * NUM_SQUARES + move.to_square\n",
        "                        score = policy_logits[0, idx].item()\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_move = move\n",
        "\n",
        "                    print(f\"ðŸ’¡ AI suggests: {best_move} (confidence: {torch.softmax(policy_logits, dim=-1)[0, idx]:.3f})\")\n",
        "                    print(f\"ðŸ“Š Position evaluation: {value.item():.3f}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                move = chess.Move.from_uci(move_input)\n",
        "                if move not in env.board.legal_moves:\n",
        "                    print(\"âŒ Illegal move! Try again.\")\n",
        "                    continue\n",
        "\n",
        "                env.board.push(move)\n",
        "                env.move_history.append(move)\n",
        "                board_state = env.board_to_tensor()\n",
        "                move_count += 1\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"âŒ Invalid move format! Use UCI notation (e.g., e2e4)\")\n",
        "                continue\n",
        "\n",
        "        else:  # AI turn\n",
        "            print(\"ðŸ¤– AI is thinking...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                policy_logits, value = model(board_state.unsqueeze(0), torch.tensor([move_count]))\n",
        "\n",
        "                # Apply softmax with temperature for more interesting play\n",
        "                temperature = 0.7\n",
        "                probs = F.softmax(policy_logits / temperature, dim=-1)\n",
        "\n",
        "                # Get legal moves and their probabilities\n",
        "                legal_moves = list(env.board.legal_moves)\n",
        "                move_probs = []\n",
        "\n",
        "                for move in legal_moves:\n",
        "                    if move.promotion and move.promotion != chess.QUEEN:\n",
        "                        continue\n",
        "                    idx = move.from_square * NUM_SQUARES + move.to_square\n",
        "                    move_probs.append((move, probs[0, idx].item()))\n",
        "\n",
        "                # Select move (top-3 sampling for more variety)\n",
        "                move_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_moves = move_probs[:3]\n",
        "                selected_move = random.choices(\n",
        "                    [m[0] for m in top_moves],\n",
        "                    weights=[m[1] for m in top_moves]\n",
        "                )[0]\n",
        "\n",
        "                env.board.push(selected_move)\n",
        "                env.move_history.append(selected_move)\n",
        "                board_state = env.board_to_tensor()\n",
        "                move_count += 1\n",
        "\n",
        "                think_time = time.time() - start_time\n",
        "                print(f\"ðŸŽ¯ AI plays: {selected_move} (thought for {think_time:.2f}s)\")\n",
        "                print(f\"ðŸ“Š Position evaluation: {value.item():.3f}\")\n",
        "\n",
        "    # Game over\n",
        "    print(f\"\\n{env.board}\")\n",
        "    result = env.board.result()\n",
        "    if result == \"1-0\":\n",
        "        print(\"ðŸŽ‰ You won! Congratulations!\")\n",
        "    elif result == \"0-1\":\n",
        "        print(\"ðŸ¤– AI won! Better luck next time!\")\n",
        "    else:\n",
        "        print(\"ðŸ¤ It's a draw!\")\n",
        "\n",
        "    print(f\"Final result: {result}\")\n",
        "\n",
        "class ChessAnalyzer:\n",
        "    \"\"\"Advanced chess position analyzer using the trained model\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, config=None):\n",
        "        if config is None:\n",
        "            config = TrainingConfig()\n",
        "\n",
        "        self.model = AdvancedChessTransformer(config).to(device)\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        self.env = ChessEnvironment()\n",
        "\n",
        "    def analyze_position(self, fen_string=None):\n",
        "        \"\"\"Analyze a chess position from FEN string or current board\"\"\"\n",
        "        if fen_string:\n",
        "            self.env.board.set_fen(fen_string)\n",
        "\n",
        "        board_state = self.env.board_to_tensor()\n",
        "        move_count = len(self.env.move_history)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy_logits, value, piece_logits, phase_logits = self.model(\n",
        "                board_state.unsqueeze(0),\n",
        "                torch.tensor([move_count]),\n",
        "                return_aux=True\n",
        "            )\n",
        "\n",
        "            # Get top moves\n",
        "            legal_moves = list(self.env.board.legal_moves)\n",
        "            move_scores = []\n",
        "\n",
        "            for move in legal_moves:\n",
        "                if move.promotion and move.promotion != chess.QUEEN:\n",
        "                    continue\n",
        "                idx = move.from_square * NUM_SQUARES + move.to_square\n",
        "                score = policy_logits[0, idx].item()\n",
        "                move_scores.append((move, score))\n",
        "\n",
        "            move_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Game phase prediction\n",
        "            phase_probs = F.softmax(phase_logits, dim=-1)[0]\n",
        "            phases = ['Opening', 'Middlegame', 'Endgame']\n",
        "            predicted_phase = phases[torch.argmax(phase_probs).item()]\n",
        "\n",
        "            analysis = {\n",
        "                'position_value': value.item(),\n",
        "                'predicted_phase': predicted_phase,\n",
        "                'phase_probabilities': {phase: prob.item() for phase, prob in zip(phases, phase_probs)},\n",
        "                'top_moves': [(str(move), score) for move, score in move_scores[:5]],\n",
        "                'turn': 'White' if self.env.board.turn else 'Black'\n",
        "            }\n",
        "\n",
        "            return analysis\n",
        "\n",
        "    def compare_positions(self, fen1, fen2):\n",
        "        \"\"\"Compare two chess positions\"\"\"\n",
        "        analysis1 = self.analyze_position(fen1)\n",
        "        analysis2 = self.analyze_position(fen2)\n",
        "\n",
        "        value_diff = analysis2['position_value'] - analysis1['position_value']\n",
        "\n",
        "        return {\n",
        "            'position1': analysis1,\n",
        "            'position2': analysis2,\n",
        "            'value_difference': value_diff,\n",
        "            'better_for': 'Position 2' if value_diff > 0 else 'Position 1' if value_diff < 0 else 'Equal'\n",
        "        }\n",
        "\n",
        "def benchmark_model(model_path, num_games=100):\n",
        "    \"\"\"Benchmark the model against random and basic players\"\"\"\n",
        "    config = TrainingConfig()\n",
        "    model = AdvancedChessTransformer(config).to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    def random_player(board):\n",
        "        moves = list(board.legal_moves)\n",
        "        return random.choice(moves) if moves else None\n",
        "\n",
        "    def greedy_player(board):\n",
        "        \"\"\"Simple greedy player that captures pieces when possible\"\"\"\n",
        "        moves = list(board.legal_moves)\n",
        "        if not moves:\n",
        "            return None\n",
        "\n",
        "        # Prioritize captures\n",
        "        captures = [m for m in moves if board.is_capture(m)]\n",
        "        if captures:\n",
        "            return random.choice(captures)\n",
        "\n",
        "        # Then checks\n",
        "        checks = [m for m in moves if board.gives_check(m)]\n",
        "        if checks:\n",
        "            return random.choice(checks)\n",
        "\n",
        "        return random.choice(moves)\n",
        "\n",
        "    def model_player(board, move_count):\n",
        "        board_state = ChessEnvironment().board_to_tensor()\n",
        "        # Update board_state based on current board\n",
        "        for square in chess.SQUARES:\n",
        "            piece = board.piece_at(square)\n",
        "            if piece is not None:\n",
        "                piece_type = piece.piece_type\n",
        "                color = piece.color\n",
        "                token = piece_type if color else piece_type + 6\n",
        "                board_state[square] = token\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy_logits, _ = model(board_state.unsqueeze(0), torch.tensor([move_count]))\n",
        "\n",
        "            legal_moves = list(board.legal_moves)\n",
        "            if not legal_moves:\n",
        "                return None\n",
        "\n",
        "            best_score = -float('inf')\n",
        "            best_move = None\n",
        "\n",
        "            for move in legal_moves:\n",
        "                if move.promotion and move.promotion != chess.QUEEN:\n",
        "                    continue\n",
        "                idx = move.from_square * NUM_SQUARES + move.to_square\n",
        "                score = policy_logits[0, idx].item()\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_move = move\n",
        "\n",
        "            return best_move\n",
        "\n",
        "    results = {'vs_random': {'wins': 0, 'losses': 0, 'draws': 0},\n",
        "               'vs_greedy': {'wins': 0, 'losses': 0, 'draws': 0}}\n",
        "\n",
        "    print(f\"ðŸŽ¯ Benchmarking model against {num_games} games each...\")\n",
        "\n",
        "    # Test against random player\n",
        "    for i in range(num_games):\n",
        "        board = chess.Board()\n",
        "        move_count = 0\n",
        "\n",
        "        while not board.is_game_over() and move_count < 200:\n",
        "            if board.turn == chess.WHITE:  # Model plays white\n",
        "                move = model_player(board, move_count)\n",
        "            else:  # Random plays black\n",
        "                move = random_player(board)\n",
        "\n",
        "            if move:\n",
        "                board.push(move)\n",
        "                move_count += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        result = board.result()\n",
        "        if result == \"1-0\":\n",
        "            results['vs_random']['wins'] += 1\n",
        "        elif result == \"0-1\":\n",
        "            results['vs_random']['losses'] += 1\n",
        "        else:\n",
        "            results['vs_random']['draws'] += 1\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"Progress: {i + 1}/{num_games} games vs random\")\n",
        "\n",
        "    # Test against greedy player\n",
        "    for i in range(num_games):\n",
        "        board = chess.Board()\n",
        "        move_count = 0\n",
        "\n",
        "        while not board.is_game_over() and move_count < 200:\n",
        "            if board.turn == chess.WHITE:  # Model plays white\n",
        "                move = model_player(board, move_count)\n",
        "            else:  # Greedy plays black\n",
        "                move = greedy_player(board)\n",
        "\n",
        "            if move:\n",
        "                board.push(move)\n",
        "                move_count += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        result = board.result()\n",
        "        if result == \"1-0\":\n",
        "            results['vs_greedy']['wins'] += 1\n",
        "        elif result == \"0-1\":\n",
        "            results['vs_greedy']['losses'] += 1\n",
        "        else:\n",
        "            results['vs_greedy']['draws'] += 1\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"Progress: {i + 1}/{num_games} games vs greedy\")\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nðŸ“Š BENCHMARK RESULTS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for opponent, stats in results.items():\n",
        "        total = sum(stats.values())\n",
        "        win_rate = stats['wins'] / total * 100 if total > 0 else 0\n",
        "        print(f\"\\n{opponent}:\")\n",
        "        print(f\"  Wins: {stats['wins']} ({stats['wins']/total*100:.1f}%)\")\n",
        "        print(f\"  Losses: {stats['losses']} ({stats['losses']/total*100:.1f}%)\")\n",
        "        print(f\"  Draws: {stats['draws']} ({stats['draws']/total*100:.1f}%)\")\n",
        "        print(f\"  Win Rate: {win_rate:.1f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_training_visualization():\n",
        "    \"\"\"Create a simple training progress visualization\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # This would be called after training to visualize progress\n",
        "    def plot_training_stats(stats):\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Rewards over time\n",
        "        axes[0, 0].plot(stats['total_rewards'])\n",
        "        axes[0, 0].set_title('Game Rewards Over Time')\n",
        "        axes[0, 0].set_xlabel('Game')\n",
        "        axes[0, 0].set_ylabel('Total Reward')\n",
        "\n",
        "        # Policy loss\n",
        "        axes[0, 1].plot(stats['policy_losses'])\n",
        "        axes[0, 1].set_title('Policy Loss Over Time')\n",
        "        axes[0, 1].set_xlabel('Training Step')\n",
        "        axes[0, 1].set_ylabel('Policy Loss')\n",
        "\n",
        "        # Value loss\n",
        "        axes[1, 0].plot(stats['value_losses'])\n",
        "        axes[1, 0].set_title('Value Loss Over Time')\n",
        "        axes[1, 0].set_xlabel('Training Step')\n",
        "        axes[1, 0].set_ylabel('Value Loss')\n",
        "\n",
        "        # Auxiliary loss\n",
        "        axes[1, 1].plot(stats['aux_losses'])\n",
        "        axes[1, 1].set_title('Auxiliary Loss Over Time')\n",
        "        axes[1, 1].set_xlabel('Training Step')\n",
        "        axes[1, 1].set_ylabel('Aux Loss')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_progress.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    return plot_training_stats\n",
        "\n",
        "# ðŸš€ MAIN EXECUTION EXAMPLES\n",
        "def main():\n",
        "    \"\"\"Main execution with different modes\"\"\"\n",
        "\n",
        "    # Configuration for publication-grade model\n",
        "    config = TrainingConfig(\n",
        "        d_model=384,      # Larger model\n",
        "        nhead=24,         # More attention heads\n",
        "        num_layers=12,    # Deeper network\n",
        "        learning_rate=1e-4,\n",
        "        batch_size=64,\n",
        "        memory_size=200000,\n",
        "        exploration_noise=0.2,\n",
        "        temperature=0.8\n",
        "    )\n",
        "\n",
        "    print(\"ðŸ§  Advanced Chess Transformer - Publication Grade\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Model parameters: ~{sum(p.numel() for p in AdvancedChessTransformer(config).parameters()):,}\")\n",
        "    print(\"Key innovations:\")\n",
        "    print(\"â€¢ Multi-scale attention (local + global patterns)\")\n",
        "    print(\"â€¢ Chess-specific positional encoding\")\n",
        "    print(\"â€¢ Game phase awareness\")\n",
        "    print(\"â€¢ Prioritized experience replay\")\n",
        "    print(\"â€¢ Self-supervised auxiliary tasks\")\n",
        "    print(\"â€¢ TD-lambda returns\")\n",
        "    print(\"â€¢ Advanced exploration strategies\")\n",
        "\n",
        "    mode = input(\"\\nSelect mode (train/play/analyze/benchmark): \").lower()\n",
        "\n",
        "    if mode == 'train':\n",
        "        trainer = AdvancedTrainer(config)\n",
        "\n",
        "        # Optional: Load from checkpoint\n",
        "        checkpoint_path = input(\"Load from checkpoint? (path or Enter to skip): \").strip()\n",
        "        if checkpoint_path:\n",
        "            try:\n",
        "                trainer.load_checkpoint(checkpoint_path)\n",
        "                print(\"âœ… Checkpoint loaded successfully!\")\n",
        "            except:\n",
        "                print(\"âŒ Could not load checkpoint, starting fresh.\")\n",
        "\n",
        "        num_games = int(input(\"Number of training games (default 2000): \") or \"2000\")\n",
        "\n",
        "        print(f\"\\nðŸš€ Starting training for {num_games} games...\")\n",
        "        model = trainer.train(num_games=num_games, save_interval=200)\n",
        "\n",
        "        print(\"âœ… Training completed!\")\n",
        "\n",
        "        # Optionally plot results (if matplotlib available)\n",
        "        try:\n",
        "            plot_fn = create_training_visualization()\n",
        "            plot_fn(trainer.stats)\n",
        "        except ImportError:\n",
        "            print(\"Install matplotlib for training visualizations\")\n",
        "\n",
        "    elif mode == 'play':\n",
        "        model_path = input(\"Model path: \").strip()\n",
        "        if not model_path:\n",
        "            print(\"âŒ Model path required!\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            play_against_human(model_path, config)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading model: {e}\")\n",
        "\n",
        "    elif mode == 'analyze':\n",
        "        model_path = input(\"Model path: \").strip()\n",
        "        if not model_path:\n",
        "            print(\"âŒ Model path required!\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            analyzer = ChessAnalyzer(model_path, config)\n",
        "\n",
        "            while True:\n",
        "                fen = input(\"\\nEnter FEN string (or 'quit'): \").strip()\n",
        "                if fen.lower() == 'quit':\n",
        "                    break\n",
        "\n",
        "                if not fen:\n",
        "                    # Use starting position\n",
        "                    fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
        "\n",
        "                analysis = analyzer.analyze_position(fen)\n",
        "\n",
        "                print(f\"\\nðŸ“Š POSITION ANALYSIS:\")\n",
        "                print(f\"Turn: {analysis['turn']}\")\n",
        "                print(f\"Evaluation: {analysis['position_value']:.3f}\")\n",
        "                print(f\"Game Phase: {analysis['predicted_phase']}\")\n",
        "                print(f\"\\nTop 5 moves:\")\n",
        "                for i, (move, score) in enumerate(analysis['top_moves'], 1):\n",
        "                    print(f\"  {i}. {move} ({score:.3f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "    elif mode == 'benchmark':\n",
        "        model_path = input(\"Model path: \").strip()\n",
        "        if not model_path:\n",
        "            print(\"âŒ Model path required!\")\n",
        "            return\n",
        "\n",
        "        num_games = int(input(\"Games per opponent (default 50): \") or \"50\")\n",
        "\n",
        "        try:\n",
        "            results = benchmark_model(model_path, num_games)\n",
        "            print(\"\\nðŸ† Benchmarking completed!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Invalid mode! Choose: train/play/analyze/benchmark\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to run\n",
        "    # main()\n",
        "\n",
        "    # Quick training example (commented out)\n",
        "    \"\"\"\n",
        "    # Quick test with smaller model\n",
        "    config = TrainingConfig(d_model=128, num_layers=4, nhead=8)\n",
        "    trainer = AdvancedTrainer(config)\n",
        "    model = trainer.train(num_games=100)\n",
        "    trainer.save_checkpoint(\"quick_test_model.pth\")\n",
        "    \"\"\"\n",
        "\n",
        "    pass"
      ]
    }
  ]
}