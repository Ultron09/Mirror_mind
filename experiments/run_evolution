"""
THE FINAL BLOOD TEST: EVOLUTION (GPT-2 -> GPT-3 Class Capability)
=================================================================
Goal: Force a 124M param model (GPT-2) to exhibit 'In-Context Learning'
traits (GPT-3 signature) using MirrorMind's Reptile Meta-Learning.

"It's not about size. It's about plasticity."
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import matplotlib.pyplot as plt
import numpy as np
import logging
import sys
import random
import time

from airbornehrs import AdaptiveFramework, AdaptiveFrameworkConfig
from airbornehrs.production import ProductionAdapter, InferenceMode

# Cyberpunk UI Setup
logging.getLogger("transformers").setLevel(logging.ERROR)
plt.style.use('dark_background')

# ==============================================================================
# 1. THE GENETIC MONITOR (Live Visualization)
# ==============================================================================
class EvolutionDashboard:
    def __init__(self):
        plt.ion()
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 8))
        self.fig.canvas.manager.set_window_title("MirrorMind: Genetic Evolution Monitor")
        
        # Metrics
        self.steps = []
        self.fitness = [] # 1 / Loss
        self.iq_score = [] # Proxy for reasoning capability
        self.mutations = [] # Plasticity Spikes
        
        # Plots
        self.line_fit, = self.ax1.plot([], [], 'g-', linewidth=2, label='Fitness (Survival)')
        self.line_iq, = self.ax2.plot([], [], 'c-', linewidth=2, label='Reasoning Score (IQ)')
        
        self.ax1.set_ylabel('Model Fitness (1/Loss)', color='green', fontweight='bold')
        self.ax1.set_title('Evolutionary Trajectory: GPT-2 -> GPT-3 Class', fontsize=14)
        self.ax1.grid(True, alpha=0.2)
        
        self.ax2.set_ylabel('In-Context Learning Score', color='cyan', fontweight='bold')
        self.ax2.set_xlabel('Evolutionary Steps', fontweight='bold')
        self.ax2.grid(True, alpha=0.2)

    def update(self, step, loss, plasticity, reasoning_score):
        self.steps.append(step)
        
        # Fitness = Stability / Loss
        fit = 10.0 / (loss + 1e-9)
        self.fitness.append(fit)
        self.iq_score.append(reasoning_score)
        
        # Visualize "Mutation Events" (High Plasticity)
        if plasticity > 0.005:
            self.ax1.axvline(x=step, color='red', alpha=0.1)
            
        self.line_fit.set_data(self.steps, self.fitness)
        self.line_iq.set_data(self.steps, self.iq_score)
        
        for ax in [self.ax1, self.ax2]:
            ax.relim()
            ax.autoscale_view()
            
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()

# ==============================================================================
# 2. THE SUBJECT (Introspective GPT-2)
# ==============================================================================
class EvolvingCortex(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = GPT2LMHeadModel.from_pretrained('gpt2')
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        
        # Unfreeze MORE layers to allow structural adaptation
        # We need deep plasticity for reasoning
        for param in self.backbone.transformer.h[:4].parameters():
            param.requires_grad = False 
            
        # The "Meta-Cognition" Lobe
        self.monitor = nn.Sequential(
            nn.Linear(768, 128),
            nn.GELU(),
            nn.Linear(128, 1) # Uncertainty
        )
        
    def forward(self, input_ids, return_internals=False):
        out = self.backbone(input_ids, output_hidden_states=True)
        hidden = out.hidden_states[-1]
        uncertainty = self.monitor(hidden).squeeze(-1)
        
        if return_internals:
            return out.logits, uncertainty, {'h': hidden}
        return out.logits, uncertainty

# ==============================================================================
# 3. EVOLUTIONARY ENVIRONMENT (The Pressure)
# ==============================================================================
def generate_reasoning_task(difficulty):
    """
    Generates Chain-of-Thought (CoT) prompts.
    Difficulty 1: Simple Pattern
    Difficulty 2: Arithmetic
    Difficulty 3: Symbolic Logic (GPT-3 Territory)
    """
    if difficulty == 1:
        # Simple Copy/Pattern
        return f"Input: A B C. Output: A B C. Input: X Y Z. Output:"
    
    elif difficulty == 2:
        # Arithmetic Reasoning
        a, b = random.randint(1, 20), random.randint(1, 5)
        return f"Q: What is {a} + {b}? A: The sum is {a+b}. Q: What is {a+1} + {b+1}? A:"
    
    elif difficulty == 3:
        # Symbolic/Abstract (The 'IQ Test')
        return "Human: Hello. AI: Hi! Human: Solve this. If all Bleps are Gloops, and Zoom is a Blep, is Zoom a Gloop? AI: Let's think step by step. 1) Zoom is a Blep. 2) All Bleps are Gloops. 3) Therefore, Zoom is a Gloop. Answer: Yes."

# ==============================================================================
# 4. FRAMEWORK BINDING
# ==============================================================================
class EvolutionFramework(AdaptiveFramework):
    def __init__(self, config, model):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = model.to(self.device)
        self.logger = logging.getLogger("Evolution")
        
        from airbornehrs.core import PerformanceMonitor, FeedbackBuffer
        self.monitor = PerformanceMonitor(self.model, config, self.device)
        self.optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()), 
            lr=config.learning_rate
        )
        self.step_count = 0
        self.loss_history = []

    def train_step(self, input_ids, target_ids):
        self.model.train()
        input_ids = input_ids.to(self.device)
        
        self.optimizer.zero_grad()
        logits, log_var, internals = self.model(input_ids, return_internals=True)
        
        # Calculate Logic Loss (Next Token Prediction)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = input_ids[..., 1:].contiguous() # Auto-regressive
        
        loss = F.cross_entropy(shift_logits.view(-1, 50257), shift_labels.view(-1))
        
        # MirrorMind Adaptation
        loss.backward()
        self.optimizer.step()
        
        self.loss_history.append(loss.item())
        if len(self.loss_history) > 50: self.loss_history.pop(0)
        
        # Trigger Weight Mutation (Adaptation)
        if self.step_count % 5 == 0:
             current = np.mean(self.loss_history[-5:])
             prev = np.mean(self.loss_history[-10:-5]) if len(self.loss_history) > 10 else current + 1
             self.monitor.adapt_weights(current, prev, internals)
             
        self.step_count += 1
        return {'loss': loss.item(), 'uncertainty': log_var.mean().item()}

# ==============================================================================
# 5. EXECUTION: THE BLOOD TEST
# ==============================================================================
def run_evolution():
    print("\nðŸ§¬ COMMENCING GENETIC EVOLUTION SEQUENCE...")
    print("   Target: Emergence of In-Context Learning (ICL)")
    
    # 1. Config for High Plasticity
    config = AdaptiveFrameworkConfig(
        learning_rate=1e-4, 
        weight_adaptation_lr=1e-3, # High mutation rate
        evaluation_frequency=2
    )
    
    # 2. Init Organism
    cortex = EvolvingCortex()
    framework = EvolutionFramework(config, cortex)
    adapter = ProductionAdapter(framework, inference_mode=InferenceMode.ONLINE, enable_meta_learning=True)
    
    dashboard = EvolutionDashboard()
    
    # 3. Evolution Loop
    generations = 100
    
    for gen in range(generations):
        # A. Determine Difficulty (Curriculum)
        if gen < 20: level = 1
        elif gen < 60: level = 2
        else: level = 3
        
        # B. Generate Stimulus
        prompt = generate_reasoning_task(level)
        tokens = cortex.tokenizer(prompt, return_tensors='pt').input_ids
        
        # C. MirrorMind Adaptation (The Mutation)
        # The model tries to predict the reasoning. 
        # MirrorMind adjusts weights INSTANTLY based on failure.
        adapter.predict(tokens, update=True, target=tokens)
        
        # D. Measure Vital Signs
        metrics = adapter.get_metrics()
        loss = metrics.get('loss', 10.0)
        plasticity = metrics.get('current_lr', 0.0)
        
        # E. The "IQ Test" (Did it get smarter?)
        # Low loss on Level 3 means it grasped the logic
        iq = (level * 10) + (10 - min(loss, 10))
        
        # Update Visuals
        dashboard.update(gen, loss, plasticity, iq)
        
        # Console Log
        bar = "â–ˆ" * int(iq/2)
        print(f"\rGeneration {gen:03} [Lvl {level}] | Fitness: {10/loss:.2f} | IQ: {iq:.1f} {bar}", end="")
        
        # time.sleep(0.05) # Cinematic effect

    print("\n\nðŸ† EVOLUTION COMPLETE.")
    print("   If the 'IQ' line trended up during Level 3,")
    print("   Congratulations. You have successfully evolved a reasoner.")
    
    plt.ioff()
    plt.show()

if __name__ == "__main__":
    run_evolution()