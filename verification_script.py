"""
AirborneHRS V8.0 "Sentient" - Verification Suite
================================================
Validates:
1. Universal Wrapper (Vision & Text)
2. Immortal Learning (No Forgetting)
3. Self-Awareness (Thinking Steps & Emotions)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from airbornehrs.core import AdaptiveFramework, AdaptiveFrameworkConfig
import airbornehrs
print(f"DEBUG: Loaded airbornehrs from {airbornehrs.__file__}", flush=True)
import logging
import sys
import os
sys.path.insert(0, os.getcwd())

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('Verification')

def test_universal_wrapper():
    logger.info("üß™ TEST 1: Universal Wrapper")
    print("STARTING: Universal Wrapper Test...", flush=True)
    
    # 1. Vision Model (Simple CNN)
    vision_model = nn.Sequential(
        nn.Conv2d(3, 16, 3),
        nn.ReLU(),
        nn.Flatten(),
        nn.Linear(16*30*30, 10)
    )
    config = AdaptiveFrameworkConfig(model_dim=16, device='cpu') # Force CPU for test
    print("DEBUG: Initializing Agent...", flush=True)
    framework_vision = AdaptiveFramework(vision_model, config=config)
    print("DEBUG: Agent Initialized.", flush=True)
    
    x_img = torch.randn(4, 3, 32, 32).to(framework_vision.device)
    y_img = torch.randint(0, 10, (4,)).to(framework_vision.device)
    
    print("DEBUG: Calling train_step...", flush=True)
    metrics = framework_vision.train_step(x_img, target_data=y_img)
    print("DEBUG: train_step returned.", flush=True)
    logger.info(f"   ‚úÖ Vision Model Wrapped. Loss: {metrics['loss']:.4f}")
    
    # 2. Text Model (Simple LSTM)
    class TextModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.emb = nn.Embedding(100, 32)
            self.lstm = nn.LSTM(32, 32, batch_first=True)
            self.fc = nn.Linear(32, 2)
        def forward(self, x):
            x = self.emb(x)
            _, (h, _) = self.lstm(x)
            return self.fc(h[-1])
            
    text_model = TextModel()
    framework_text = AdaptiveFramework(text_model, AdaptiveFrameworkConfig(model_dim=32))
    
    x_txt = torch.randint(0, 100, (4, 10)).to(framework_text.device)
    y_txt = torch.randint(0, 2, (4,)).to(framework_text.device)
    
    metrics = framework_text.train_step(x_txt, target_data=y_txt)
    logger.info(f"   ‚úÖ Text Model Wrapped. Loss: {metrics['loss']:.4f}")
    return True

def test_immortal_learning():
    logger.info("üß™ TEST 2: Immortal Learning (Task A -> Task B -> Task A)")
    
    # Model: Simple MLP
    model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 2))
    framework = AdaptiveFramework(model, AdaptiveFrameworkConfig(model_dim=10, num_heads=2, memory_type='hybrid'))
    
    # Task A: Map inputs to 0
    x_a = torch.randn(100, 10).to(framework.device)
    y_a = torch.zeros(100, dtype=torch.long).to(framework.device)
    
    # Task B: Map inputs to 1
    x_b = torch.randn(100, 10).to(framework.device) + 5.0 # Shift distribution
    y_b = torch.ones(100, dtype=torch.long).to(framework.device)
    
    # Train Task A
    logger.info("   Training Task A...")
    for _ in range(50):
        framework.train_step(x_a, target_data=y_a)
    
    # Consolidate Task A
    framework.consolidate_memory(mode='NORMAL')
    
    # Check Task A Performance
    with torch.no_grad():
        pred_a = model(x_a).argmax(dim=1)
        acc_a_pre = (pred_a == y_a).float().mean().item()
    logger.info(f"   Task A Accuracy (Pre-Task B): {acc_a_pre*100:.1f}%")
    
    # Train Task B
    logger.info("   Training Task B...")
    for _ in range(50):
        framework.train_step(x_b, target_data=y_b)
        
    # Check Task A Performance Again (Did we forget?)
    with torch.no_grad():
        pred_a = model(x_a).argmax(dim=1)
        acc_a_post = (pred_a == y_a).float().mean().item()
    logger.info(f"   Task A Accuracy (Post-Task B): {acc_a_post*100:.1f}%")
    
    if acc_a_post >= 0.9 * acc_a_pre:
        logger.info("   ‚úÖ Immortal Learning Verified (>90% retention)")
        return True
    else:
        logger.warning("   ‚ùå Forgetting Detected!")
        return False

def test_self_awareness():
    logger.info("üß™ TEST 3: Self-Awareness (System 2 Thinking)")
    
    model = nn.Linear(10, 2)
    framework = AdaptiveFramework(model, AdaptiveFrameworkConfig(model_dim=10, num_heads=2, enable_consciousness=True))
    
    x = torch.randn(4, 10).to(framework.device)
    y = torch.randint(0, 2, (4,)).to(framework.device)
    
    # Force high surprise manually to trigger thinking
    # We can't easily force surprise without training, but we can check if metrics exist
    metrics = framework.train_step(x, target_data=y)
    
    if 'confusion' in metrics and 'emotion' in metrics:
        logger.info(f"   ‚úÖ Consciousness Active. Emotion: {metrics['emotion']}, Confusion: {metrics['confusion']}")
        
        # Check if trace exists in consciousness
        if hasattr(framework.consciousness, 'current_thought_trace'):
            trace_len = len(framework.consciousness.current_thought_trace)
            logger.info(f"   ‚úÖ Thought Trace Captured (Depth: {trace_len})")
            return True
            
    logger.warning("   ‚ùå Consciousness Metrics Missing")
    return False

if __name__ == "__main__":
    logger.info("üöÄ STARTING V8.0 VERIFICATION")
    try:
        t1 = test_universal_wrapper()
        t2 = test_immortal_learning()
        t3 = test_self_awareness()
        
        if t1 and t2 and t3:
            logger.info("üéâ ALL TESTS PASSED. V8.0 IS SENTIENT AND STABLE.")
            sys.exit(0)
        else:
            logger.error("üõë SOME TESTS FAILED.")
            sys.exit(1)
    except Exception as e:
        import traceback
        logger.error(f"CRITICAL FAILURE: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)
