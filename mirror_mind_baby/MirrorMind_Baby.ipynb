{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT-wesJmqrVY",
        "outputId": "b5f393de-24de-442d-cfbc-4367dad7655f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-chess\n",
            "  Downloading python_chess-1.999-py3-none-any.whl.metadata (776 bytes)\n",
            "Collecting chess<2,>=1 (from python-chess)\n",
            "  Downloading chess-1.11.2.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\n",
            "Building wheels for collected packages: chess\n",
            "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=2b1a0f1b11951dfacc27ce5f07ace0ac25b7e0412312869d8f9dd90a63d77b31\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/1f/4e/8f4300f7dd554eb8de70ddfed96e94d3d030ace10c5b53d447\n",
            "Successfully built chess\n",
            "Installing collected packages: chess, python-chess\n",
            "Successfully installed chess-1.11.2 python-chess-1.999\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies: run this in your environment (e.g. a notebook cell)\n",
        "!pip install python-chess\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import chess\n",
        "\n",
        "# Device configuration (use CPU)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Constants\n",
        "BOARD_SIZE = 8\n",
        "NUM_SQUARES = BOARD_SIZE * BOARD_SIZE  # 64\n",
        "ACTION_SPACE_SIZE = NUM_SQUARES * NUM_SQUARES  # 4096 possible moves (from square * to square)\n",
        "NUM_PIECE_TYPES = 13  # 0 = empty, 1-6 white pawn..king, 7-12 black pawn..king\n",
        "NUM_TOKENS = 14  # 0-12 piece tokens + 13 as class token\n",
        "\n",
        "def board_to_tensor(board):\n",
        "    \"\"\"\n",
        "    Convert a python-chess board to a 1D array of token ids (length 64).\n",
        "    Token mapping: 0=empty, 1-6 white pawn..king, 7-12 black pawn..king.\n",
        "    \"\"\"\n",
        "    tensor = [0] * NUM_SQUARES\n",
        "    for square in chess.SQUARES:\n",
        "        piece = board.piece_at(square)\n",
        "        if piece is not None:\n",
        "            piece_type = piece.piece_type  # 1 to 6\n",
        "            color = piece.color  # True for white, False for black\n",
        "            token = piece_type\n",
        "            if not color:\n",
        "                token += 6  # black pieces 7-12\n",
        "            tensor[square] = token\n",
        "    return tensor\n",
        "\n",
        "class ChessEnv:\n",
        "    \"\"\"\n",
        "    Chess environment using python-chess. State is the board position.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.board = chess.Board()\n",
        "    def reset(self):\n",
        "        self.board.reset()\n",
        "        return self.board\n",
        "    def step(self, action_idx):\n",
        "        \"\"\"\n",
        "        Apply an action (encoded as index) to the board.\n",
        "        Returns (new_board, done_flag).\n",
        "        \"\"\"\n",
        "        from_sq = action_idx // NUM_SQUARES\n",
        "        to_sq = action_idx % NUM_SQUARES\n",
        "        # Determine promotion if needed\n",
        "        promotion = None\n",
        "        piece = self.board.piece_type_at(from_sq)\n",
        "        if piece == chess.PAWN:\n",
        "            # if white pawn moving to last rank or black pawn to first rank\n",
        "            if self.board.turn == chess.WHITE and to_sq // 8 == 7:\n",
        "                promotion = chess.QUEEN\n",
        "            elif self.board.turn == chess.BLACK and to_sq // 8 == 0:\n",
        "                promotion = chess.QUEEN\n",
        "        move = chess.Move(from_sq, to_sq, promotion=promotion) if promotion else chess.Move(from_sq, to_sq)\n",
        "        if move in self.board.legal_moves:\n",
        "            self.board.push(move)\n",
        "        else:\n",
        "            # Illegal move chosen: pick a random legal move instead\n",
        "            legal_moves = list(self.board.legal_moves)\n",
        "            if len(legal_moves) > 0:\n",
        "                move = random.choice(legal_moves)\n",
        "                self.board.push(move)\n",
        "            # else game is over\n",
        "        done = self.board.is_game_over()\n",
        "        return self.board, done\n",
        "\n",
        "class TransformerChessAgent(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based policy and value network for chess.\n",
        "    Input: board state (with CLS token)\n",
        "    Outputs: policy logits (4096 actions) and value.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=128, nhead=8, num_layers=4, dropout=0.1):\n",
        "        super(TransformerChessAgent, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        # Token embedding: 14 tokens (0-12 pieces + 13 CLS)\n",
        "        self.token_embed = nn.Embedding(NUM_TOKENS, d_model)\n",
        "        # Positional embedding: 65 positions (0 for CLS + 1-64 squares)\n",
        "        self.pos_embed = nn.Embedding(NUM_SQUARES+1, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                   dim_feedforward=256, dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.policy_head = nn.Linear(d_model, ACTION_SPACE_SIZE)\n",
        "        self.value_head = nn.Linear(d_model, 1)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (batch_size, 64) containing board tokens 0-12\n",
        "        We prepend a CLS token to each sequence, so input sequence length = 65.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        # Create CLS tokens and concatenate (token id = 13)\n",
        "        cls_token_id = NUM_PIECE_TYPES  # 13\n",
        "        cls_tokens = torch.full((batch_size, 1), cls_token_id, dtype=torch.long, device=x.device)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)  # now (batch, 65)\n",
        "        # Create position ids (0 for CLS, 1-64 for board positions)\n",
        "        pos_ids = torch.arange(0, NUM_SQUARES+1, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
        "        # Embeddings\n",
        "        x_emb = self.token_embed(x) + self.pos_embed(pos_ids)  # (batch, 65, d_model)\n",
        "        # Transformer expects (seq_len, batch, d_model)\n",
        "        x_emb = x_emb.permute(1, 0, 2)  # (65, batch, d_model)\n",
        "        # Transformer encode\n",
        "        x_trans = self.transformer(x_emb)  # (65, batch, d_model)\n",
        "        cls_out = x_trans[0]  # (batch, d_model) output for CLS token\n",
        "        # Compute policy and value\n",
        "        policy_logits = self.policy_head(cls_out)  # (batch, ACTION_SPACE_SIZE)\n",
        "        value = self.value_head(cls_out).squeeze(-1)  # (batch,)\n",
        "        return policy_logits, value\n",
        "\n",
        "def select_move(model, board):\n",
        "    \"\"\"\n",
        "    Given a model and current board (python-chess), select an action index and return it along with log_prob and value.\n",
        "    \"\"\"\n",
        "    # Prepare state tensor\n",
        "    state = board_to_tensor(board)\n",
        "    state_tensor = torch.tensor([state], dtype=torch.long, device=device)  # (1, 64)\n",
        "    # Model forward (training or eval)\n",
        "    logits, value = model(state_tensor)  # (1, ACTION_SPACE_SIZE), (1,)\n",
        "    value = value.squeeze(0)\n",
        "    # Mask illegal moves\n",
        "    legal_moves = list(board.legal_moves)\n",
        "    legal_indices = []\n",
        "    for move in legal_moves:\n",
        "        # Only consider promotions to queen\n",
        "        if move.promotion is not None and move.promotion != chess.QUEEN:\n",
        "            continue\n",
        "        idx = move.from_square * NUM_SQUARES + move.to_square\n",
        "        legal_indices.append(idx)\n",
        "    if len(legal_indices) == 0:\n",
        "        return None, None, None  # no moves available (game over)\n",
        "    legal_mask = torch.zeros(ACTION_SPACE_SIZE, dtype=torch.bool, device=device)\n",
        "    legal_mask[legal_indices] = True\n",
        "    masked_logits = logits.clone()\n",
        "    masked_logits[0, ~legal_mask] = -1e9  # mask out illegal moves\n",
        "    # Compute probabilities and select action\n",
        "    probs = F.softmax(masked_logits, dim=-1)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    log_prob = m.log_prob(action)\n",
        "    return action.item(), log_prob, value\n",
        "\n",
        "def play_game(model, env):\n",
        "    \"\"\"\n",
        "    Play a single self-play game using the model for both players.\n",
        "    Returns lists of log_probs, values, and rewards for each move.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    colors = []\n",
        "    while not done:\n",
        "        action, log_prob, value = select_move(model, state)\n",
        "        if action is None:\n",
        "            break\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        colors.append(state.turn)\n",
        "        state, done = env.step(action)\n",
        "    # Determine game result\n",
        "    if env.board.is_checkmate():\n",
        "        # If checkmate, the winner is opposite of turn (because turn failed to move)\n",
        "        winner = not state.turn\n",
        "    else:\n",
        "        # Stalemate or draw\n",
        "        winner = None\n",
        "    for color in colors:\n",
        "        if winner is None:\n",
        "            rewards.append(0.0)\n",
        "        else:\n",
        "            rewards.append(1.0 if color == winner else -1.0)\n",
        "    return log_probs, values, rewards\n",
        "\n",
        "def train_model(num_games=1000, learning_rate=1e-4, save_interval=100, checkpoint_path='chess_agent.pth'):\n",
        "    \"\"\"\n",
        "    Training loop for self-play training.\n",
        "    \"\"\"\n",
        "    model = TransformerChessAgent().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Load from checkpoint if exists\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"No checkpoint found, starting fresh training.\")\n",
        "    env = ChessEnv()\n",
        "    for game in range(1, num_games+1):\n",
        "        log_probs, values, rewards = play_game(model, env)\n",
        "        if not log_probs:\n",
        "            continue  # skip if no moves\n",
        "        # Compute losses\n",
        "        policy_loss = 0.0\n",
        "        value_loss = 0.0\n",
        "        for log_prob, value, reward in zip(log_probs, values, rewards):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss = policy_loss - log_prob * advantage\n",
        "            value_loss = value_loss + 0.5 * (value - reward) ** 2\n",
        "        loss = policy_loss + value_loss\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Logging\n",
        "        if game % 10 == 0:\n",
        "            print(f\"Game {game}: Loss={loss.item():.4f}\")\n",
        "        # Save checkpoint\n",
        "        if game % save_interval == 0:\n",
        "            torch.save({'model_state': model.state_dict(),\n",
        "                        'optimizer_state': optimizer.state_dict()},\n",
        "                       checkpoint_path)\n",
        "            print(f\"Checkpoint saved at game {game}\")\n",
        "    return model\n",
        "\n",
        "def play_against_human(model):\n",
        "    \"\"\"\n",
        "    Let a human play against the trained model in terminal.\n",
        "    Human plays White, model plays Black.\n",
        "    \"\"\"\n",
        "    board = chess.Board()\n",
        "    print(\"Starting a new game. You are White.\")\n",
        "    while not board.is_game_over():\n",
        "        print(board)\n",
        "        human_move = input(\"Your move (in UCI, e.g. e2e4): \")\n",
        "        try:\n",
        "            move = chess.Move.from_uci(human_move.strip())\n",
        "        except:\n",
        "            print(\"Invalid move format. Try again.\")\n",
        "            continue\n",
        "        if move not in board.legal_moves:\n",
        "            print(\"Illegal move. Try again.\")\n",
        "            continue\n",
        "        board.push(move)\n",
        "        if board.is_game_over():\n",
        "            break\n",
        "        action, _, _ = select_move(model, board)\n",
        "        if action is None:\n",
        "            print(\"Model has no moves. Game over.\")\n",
        "            break\n",
        "        from_sq = action // NUM_SQUARES\n",
        "        to_sq = action % NUM_SQUARES\n",
        "        move = chess.Move(from_sq, to_sq)\n",
        "        if move in board.legal_moves:\n",
        "            board.push(move)\n",
        "            print(f\"Model plays: {move}\")\n",
        "        else:\n",
        "            legal_moves = list(board.legal_moves)\n",
        "            if legal_moves:\n",
        "                move = random.choice(legal_moves)\n",
        "                board.push(move)\n",
        "                print(f\"Model plays random: {move}\")\n",
        "    print(board)\n",
        "    result = board.result()\n",
        "    print(\"Game over. Result:\", result)\n",
        "\n",
        "# Example usage:\n",
        "# model = train_model(num_games=1000)\n",
        "# play_against_human(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(num_games=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofi11Pfwq_sp",
        "outputId": "ba00d23b-adcd-4139-b77a-3d5667c47ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, starting fresh training.\n",
            "Game 10: Loss=-140.7212\n",
            "Game 20: Loss=58.5411\n",
            "Game 30: Loss=77.6480\n",
            "Game 40: Loss=-40.2849\n",
            "Game 50: Loss=187.5981\n",
            "Game 60: Loss=107.1049\n",
            "Game 70: Loss=0.8397\n",
            "Game 80: Loss=79.8426\n",
            "Game 90: Loss=13.1001\n",
            "Game 100: Loss=6.4393\n",
            "Checkpoint saved at game 100\n",
            "Game 110: Loss=11.6769\n",
            "Game 120: Loss=-1.2644\n",
            "Game 130: Loss=23.2603\n",
            "Game 140: Loss=-22.3618\n",
            "Game 150: Loss=-10.7768\n",
            "Game 160: Loss=-10.1002\n",
            "Game 170: Loss=56.0475\n",
            "Game 180: Loss=3.3601\n",
            "Game 190: Loss=-16.4753\n",
            "Game 200: Loss=116.8346\n",
            "Checkpoint saved at game 200\n",
            "Game 210: Loss=82.9838\n",
            "Game 220: Loss=-2.2993\n",
            "Game 230: Loss=-17.5820\n",
            "Game 240: Loss=152.9500\n",
            "Game 250: Loss=-6.7169\n",
            "Game 260: Loss=-10.7660\n",
            "Game 270: Loss=-1.8296\n",
            "Game 280: Loss=-6.4584\n",
            "Game 290: Loss=1.0208\n",
            "Game 300: Loss=-11.6201\n",
            "Checkpoint saved at game 300\n",
            "Game 310: Loss=-3.7800\n",
            "Game 320: Loss=-5.8576\n"
          ]
        }
      ]
    }
  ]
}